{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMSDPXwgCGs/mo6oHkzEKcd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nRieNBiirdhV","executionInfo":{"status":"ok","timestamp":1768413955801,"user_tz":-330,"elapsed":40049,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"c2704cd9-2121-43e3-99f7-b32f7178d853"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/ML/DL_With_Pytorch\n","From https://github.com/barada02/DL_With_Pytorch\n"," * branch            main       -> FETCH_HEAD\n","Already up to date.\n","✅ Environment Ready!\n"]}],"source":["# Using Drive as storage and github for version controll.\n","\n","from google.colab import drive, userdata\n","import os\n","\n","# 1. Mount Drive\n","drive.mount('/content/drive')\n","\n","# 2. Setup Paths (Change to your actual repo name)\n","REPO_PATH = \"/content/drive/MyDrive/ML/DL_With_Pytorch\"\n","%cd {REPO_PATH}\n","\n","# 3. Secure Auth\n","token = userdata.get('GH_TOKEN')\n","username = \"barada02\"\n","repo = \"DL_With_Pytorch\"\n","!git remote set-url origin https://{token}@github.com/{username}/{repo}.git\n","\n","# 4. Identity\n","!git config --global user.email \"Chandanbarada2@gmail.com\"\n","!git config --global user.name \"Kumar\"\n","\n","!git pull origin main\n","print(\"✅ Environment Ready!\")"]},{"cell_type":"code","source":["# Push notebook changes to GitHub\n","# IMPORTANT: Press Ctrl+S (Save) before running this!\n","!git add .\n","!git commit -m \"Transpose and Permute use\"\n","!git push origin main"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dUILGGWfrue1","executionInfo":{"status":"ok","timestamp":1768414597457,"user_tz":-330,"elapsed":2074,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"98e09585-a20b-4da0-ba89-554e08e58f5c"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["[main a62df99] Transpose and Permute use\n"," 1 file changed, 1 insertion(+), 1 deletion(-)\n"," rewrite 01_Tensor_03_Shapes.ipynb (80%)\n","Enumerating objects: 5, done.\n","Counting objects: 100% (5/5), done.\n","Delta compression using up to 2 threads\n","Compressing objects: 100% (3/3), done.\n","Writing objects: 100% (3/3), 480 bytes | 96.00 KiB/s, done.\n","Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\n","remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n","To https://github.com/barada02/DL_With_Pytorch.git\n","   b35fb12..a62df99  main -> main\n"]}]},{"cell_type":"markdown","source":["# Tensor Shapes"],"metadata":{"id":"K8Bc8ihwr_XQ"}},{"cell_type":"markdown","source":["Manipulating tensor shapes is the bread and butter of deep learning. In PyTorch, these operations allow you to reorganize data to fit the expected input of different layers (like moving from a Convolutional layer to a Linear one)"],"metadata":{"id":"hp-rNi11r_UC"}},{"cell_type":"code","source":["import torch\n","torch.__version__\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"QpCvVfcNs0M9","executionInfo":{"status":"ok","timestamp":1768414219665,"user_tz":-330,"elapsed":109,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"6b9b0fb8-f1d8-4b65-9cf1-e7cfbd4461cb"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.9.0+cpu'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["# The flatten\n","### Reshape and view"],"metadata":{"id":"ue7mjZqEscTQ"}},{"cell_type":"markdown","source":["Both change the dimensions of a tensor without changing its data, but they handle memory differently.\n","\n","* view(): The OG method. It is very fast because it creates a \"view\" of the original data without copying it. Requirement: The tensor must be contiguous in memory. If you’ve just transposed a tensor, view() might throw an error.\n","\n","* reshape(): The more robust sibling. It tries to return a view if possible, but if the data isn't contiguous, it will silently copy the data to a new memory block.\n","\n","Pro Tip: Use -1 as a dimension to let PyTorch automatically calculate the size for that slot based on the remaining dimensions."],"metadata":{"id":"2UwCZyUrsiA9"}},{"cell_type":"code","source":["\n","\n","# A batch of 4 images, 3 color channels, 28x28 pixels\n","x = torch.randn(4, 3, 28, 28)\n","print(f\"Original shape: {x.shape}\")\n","\n","# Use -1 to say: \"Keep the batch size (4), but squash everything else\"\n","flattened = x.view(4, -1)\n","print(f\"Flattened shape: {flattened.shape}\") # [4, 2352]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pl5b-fIUsMns","executionInfo":{"status":"ok","timestamp":1768414237359,"user_tz":-330,"elapsed":119,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"c12f6507-ea90-4281-8e9a-af0e4a9fe9e4"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Original shape: torch.Size([4, 3, 28, 28])\n","Flattened shape: torch.Size([4, 2352])\n"]}]},{"cell_type":"markdown","source":["# Squeeze and Unsqueeze\n","These are used to add or remove \"singleton\" dimensions (dimensions of size 1).\n","\n","* ```unsqueeze(dim)```: Adds a dimension of size 1 at the specified index.\n","\n","  * Example: Changing a shape from [3, 224, 224] to [1, 3, 224, 224] to add a \"batch\" dimension.\n","\n","* ```squeeze(dim)```: Removes a dimension of size 1. If no dimension is specified, it removes all dimensions of size 1.\n","\n","   * Example: Changing [1, 10] to [10]."],"metadata":{"id":"6tQ6A6yGtEHj"}},{"cell_type":"markdown","source":["> **PyTorch models usually expect a Batch dimension. If you have a single image, you have to \"fake\" a batch of one.**"],"metadata":{"id":"dRUzWFsKtqNX"}},{"cell_type":"code","source":["# A single RGB image\n","img = torch.randn(3, 224, 224)\n","print(f\"Single image: {img.shape}\")\n","\n","# Add a batch dimension at index 0\n","batch_img = img.unsqueeze(0)\n","print(f\"After unsqueeze(0): {batch_img.shape}\") # [1, 3, 224, 224]\n","\n","# Remove it back\n","back_to_img = batch_img.squeeze(0)\n","print(f\"After squeeze(0): {back_to_img.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I25dRazhtZi8","executionInfo":{"status":"ok","timestamp":1768414361625,"user_tz":-330,"elapsed":85,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"389c9d6c-194f-43a9-851b-3bc1dd31223d"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Single image: torch.Size([3, 224, 224])\n","After unsqueeze(0): torch.Size([1, 3, 224, 224])\n","After squeeze(0): torch.Size([3, 224, 224])\n"]}]},{"cell_type":"markdown","source":["# 3. Transpose and Permute\n","These operations swap the axes of the tensor.\n","\n","* transpose(dim0, dim1): Swaps exactly two dimensions. This is common in NLP or for simple matrix flips.\n","\n","* permute(dims): A more powerful version of transpose that can reorder any number of dimensions at once.\n","\n","  * Example: Converting an image from HWC (Height, Width, Channels) to CHW format: tensor.permute(2, 0, 1)."],"metadata":{"id":"V9QBiri4t4Kf"}},{"cell_type":"code","source":["# Typical PyTorch tensor: [Channels, Height, Width]\n","img_tensor = torch.randn(3, 128, 128)\n","\n","# Permute to [Height, Width, Channels] for plotting\n","img_for_plt = img_tensor.permute(1, 2, 0)\n","print(f\"Permuted for plotting: {img_for_plt.shape}\") # [128, 128, 3]\n","\n","# Transpose is usually for just two specific dims\n","weight_matrix = torch.randn(10, 5)\n","transposed = weight_matrix.transpose(0, 1)\n","print(f\"Transposed matrix: {transposed.shape}\") # [5, 10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BQCTOMBTuEfP","executionInfo":{"status":"ok","timestamp":1768414537732,"user_tz":-330,"elapsed":72,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"3b943a5b-94ca-4820-db60-7f1ff81a9b64"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Permuted for plotting: torch.Size([128, 128, 3])\n","Transposed matrix: torch.Size([5, 10])\n"]}]},{"cell_type":"markdown","source":["> **This is vital because libraries like Matplotlib expect images in (H, W, C) format, but PyTorch uses (C, H, W).**"],"metadata":{"id":"tEvxGVG5uPpj"}},{"cell_type":"code","source":[],"metadata":{"id":"itRPFU3iuSV0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Erro Scopido\n","## The \"Contiguous\" Trap\n","If you see the error RuntimeError: view size is not compatible with input tensor's size and stride, it's because you tried to view a tensor after transposing it."],"metadata":{"id":"HROhZJDsuYbB"}}]}