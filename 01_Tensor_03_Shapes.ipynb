{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOWUrHyecoWbGIswfS0cpPM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nRieNBiirdhV","executionInfo":{"status":"ok","timestamp":1768413955801,"user_tz":-330,"elapsed":40049,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"c2704cd9-2121-43e3-99f7-b32f7178d853"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/ML/DL_With_Pytorch\n","From https://github.com/barada02/DL_With_Pytorch\n"," * branch            main       -> FETCH_HEAD\n","Already up to date.\n","âœ… Environment Ready!\n"]}],"source":["# Using Drive as storage and github for version controll.\n","\n","from google.colab import drive, userdata\n","import os\n","\n","# 1. Mount Drive\n","drive.mount('/content/drive')\n","\n","# 2. Setup Paths (Change to your actual repo name)\n","REPO_PATH = \"/content/drive/MyDrive/ML/DL_With_Pytorch\"\n","%cd {REPO_PATH}\n","\n","# 3. Secure Auth\n","token = userdata.get('GH_TOKEN')\n","username = \"barada02\"\n","repo = \"DL_With_Pytorch\"\n","!git remote set-url origin https://{token}@github.com/{username}/{repo}.git\n","\n","# 4. Identity\n","!git config --global user.email \"Chandanbarada2@gmail.com\"\n","!git config --global user.name \"Kumar\"\n","\n","!git pull origin main\n","print(\"âœ… Environment Ready!\")"]},{"cell_type":"markdown","source":["# Tensor Shapes"],"metadata":{"id":"K8Bc8ihwr_XQ"}},{"cell_type":"markdown","source":["Manipulating tensor shapes is the bread and butter of deep learning. In PyTorch, these operations allow you to reorganize data to fit the expected input of different layers (like moving from a Convolutional layer to a Linear one)"],"metadata":{"id":"hp-rNi11r_UC"}},{"cell_type":"code","source":["import torch\n","torch.__version__\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"QpCvVfcNs0M9","executionInfo":{"status":"ok","timestamp":1768414219665,"user_tz":-330,"elapsed":109,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"6b9b0fb8-f1d8-4b65-9cf1-e7cfbd4461cb"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.9.0+cpu'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["# The flatten\n","### Reshape and view"],"metadata":{"id":"ue7mjZqEscTQ"}},{"cell_type":"markdown","source":["Both change the dimensions of a tensor without changing its data, but they handle memory differently.\n","\n","* view(): The OG method. It is very fast because it creates a \"view\" of the original data without copying it. Requirement: The tensor must be contiguous in memory. If youâ€™ve just transposed a tensor, view() might throw an error.\n","\n","* reshape(): The more robust sibling. It tries to return a view if possible, but if the data isn't contiguous, it will silently copy the data to a new memory block.\n","\n","> **`Pro Tip:` Use -1 as a dimension to let PyTorch automatically calculate the size for that slot based on the remaining dimensions.**"],"metadata":{"id":"2UwCZyUrsiA9"}},{"cell_type":"code","source":["\n","\n","# A batch of 4 images, 3 color channels, 28x28 pixels\n","x = torch.randn(4, 3, 28, 28)\n","print(f\"Original shape: {x.shape}\")\n","\n","# Use -1 to say: \"Keep the batch size (4), but squash everything else\"\n","flattened = x.view(4, -1)\n","print(f\"Flattened shape: {flattened.shape}\") # [4, 2352]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pl5b-fIUsMns","executionInfo":{"status":"ok","timestamp":1768414237359,"user_tz":-330,"elapsed":119,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"c12f6507-ea90-4281-8e9a-af0e4a9fe9e4"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Original shape: torch.Size([4, 3, 28, 28])\n","Flattened shape: torch.Size([4, 2352])\n"]}]},{"cell_type":"markdown","source":["> **You will use this constantly when moving from Convolutional layers (3D) to Linear layers (1D).**\n","\n","\n"],"metadata":{"id":"R6dXSQv_vhAX"}},{"cell_type":"markdown","source":["# Squeeze and Unsqueeze\n","These are used to add or remove \"singleton\" dimensions (dimensions of size 1).\n","\n","* ```unsqueeze(dim)```: Adds a dimension of size 1 at the specified index.\n","\n","  * Example: Changing a shape from [3, 224, 224] to [1, 3, 224, 224] to add a \"batch\" dimension.\n","\n","* ```squeeze(dim)```: Removes a dimension of size 1. If no dimension is specified, it removes all dimensions of size 1.\n","\n","   * Example: Changing [1, 10] to [10]."],"metadata":{"id":"6tQ6A6yGtEHj"}},{"cell_type":"markdown","source":["> **PyTorch models usually expect a Batch dimension. If you have a single image, you have to \"fake\" a batch of one.**"],"metadata":{"id":"dRUzWFsKtqNX"}},{"cell_type":"code","source":["# A single RGB image\n","img = torch.randn(3, 224, 224)\n","print(f\"Single image: {img.shape}\")\n","\n","# Add a batch dimension at index 0\n","batch_img = img.unsqueeze(0)\n","print(f\"After unsqueeze(0): {batch_img.shape}\") # [1, 3, 224, 224]\n","\n","# Remove it back\n","back_to_img = batch_img.squeeze(0)\n","print(f\"After squeeze(0): {back_to_img.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I25dRazhtZi8","executionInfo":{"status":"ok","timestamp":1768414361625,"user_tz":-330,"elapsed":85,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"389c9d6c-194f-43a9-851b-3bc1dd31223d"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Single image: torch.Size([3, 224, 224])\n","After unsqueeze(0): torch.Size([1, 3, 224, 224])\n","After squeeze(0): torch.Size([3, 224, 224])\n"]}]},{"cell_type":"markdown","source":["# 3. Transpose and Permute\n","These operations swap the axes of the tensor.\n","\n","* transpose(dim0, dim1): Swaps exactly two dimensions. This is common in NLP or for simple matrix flips.\n","\n","* permute(dims): A more powerful version of transpose that can reorder any number of dimensions at once.\n","\n","  * Example: Converting an image from HWC (Height, Width, Channels) to CHW format: tensor.permute(2, 0, 1)."],"metadata":{"id":"V9QBiri4t4Kf"}},{"cell_type":"code","source":["# Typical PyTorch tensor: [Channels, Height, Width]\n","img_tensor = torch.randn(3, 128, 128)\n","\n","# Permute to [Height, Width, Channels] for plotting\n","img_for_plt = img_tensor.permute(1, 2, 0)\n","print(f\"Permuted for plotting: {img_for_plt.shape}\") # [128, 128, 3]\n","\n","# Transpose is usually for just two specific dims\n","weight_matrix = torch.randn(10, 5)\n","transposed = weight_matrix.transpose(0, 1)\n","print(f\"Transposed matrix: {transposed.shape}\") # [5, 10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BQCTOMBTuEfP","executionInfo":{"status":"ok","timestamp":1768414537732,"user_tz":-330,"elapsed":72,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"3b943a5b-94ca-4820-db60-7f1ff81a9b64"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Permuted for plotting: torch.Size([128, 128, 3])\n","Transposed matrix: torch.Size([5, 10])\n"]}]},{"cell_type":"markdown","source":["> **This is vital because libraries like Matplotlib expect images in (H, W, C) format, but PyTorch uses (C, H, W).**"],"metadata":{"id":"tEvxGVG5uPpj"}},{"cell_type":"markdown","source":["## Random tensor to image"],"metadata":{"id":"-ZS4tyjJzUg0"}},{"cell_type":"code","source":["import torch\n","import matplotlib.pyplot as plt\n","\n","# 1. Generate a random tensor [Channels, Height, Width]\n","# Values will be between 0 and 1\n","random_image_tensor = torch.rand(3, 7, 7)\n","print(random_image_tensor)\n","# 2. Matplotlib expects [Height, Width, Channels]\n","# We use .permute() to swap the axes around\n","vis_tensor = random_image_tensor.permute(1, 2, 0)\n","\n","# 3. Display it\n","plt.imshow(vis_tensor)\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":829},"id":"itRPFU3iuSV0","executionInfo":{"status":"ok","timestamp":1768415900612,"user_tz":-330,"elapsed":733,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"8a1e6134-6d42-4b8a-b621-e5a36bcb2504"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[0.5339, 0.1701, 0.7149, 0.6114, 0.0241, 0.2133, 0.8245],\n","         [0.1234, 0.4643, 0.4225, 0.9595, 0.9239, 0.6427, 0.8463],\n","         [0.7191, 0.9437, 0.3362, 0.4947, 0.6761, 0.2072, 0.5605],\n","         [0.1133, 0.7685, 0.5294, 0.1786, 0.1817, 0.0521, 0.6990],\n","         [0.0429, 0.1854, 0.4721, 0.5213, 0.6097, 0.4637, 0.6590],\n","         [0.9388, 0.4291, 0.8725, 0.8574, 0.5778, 0.5121, 0.8953],\n","         [0.1561, 0.9382, 0.0415, 0.6093, 0.0617, 0.2916, 0.3687]],\n","\n","        [[0.3339, 0.7196, 0.2785, 0.6824, 0.6971, 0.4927, 0.1587],\n","         [0.4449, 0.0468, 0.3031, 0.6979, 0.8752, 0.8994, 0.8754],\n","         [0.1522, 0.6897, 0.2659, 0.1023, 0.5379, 0.2059, 0.1105],\n","         [0.3277, 0.7597, 0.6443, 0.5257, 0.7569, 0.1005, 0.7972],\n","         [0.0952, 0.2559, 0.1362, 0.3559, 0.4365, 0.1276, 0.9333],\n","         [0.6111, 0.5863, 0.5758, 0.3050, 0.8233, 0.5202, 0.4542],\n","         [0.0211, 0.9589, 0.0735, 0.5307, 0.9761, 0.1252, 0.9021]],\n","\n","        [[0.8928, 0.7587, 0.7667, 0.0129, 0.0530, 0.1921, 0.7187],\n","         [0.6622, 0.6090, 0.7353, 0.0795, 0.4589, 0.6996, 0.8435],\n","         [0.1617, 0.1440, 0.0103, 0.7655, 0.8449, 0.4627, 0.4930],\n","         [0.3631, 0.3517, 0.3941, 0.3577, 0.7676, 0.7094, 0.7852],\n","         [0.2619, 0.2837, 0.0062, 0.8292, 0.8432, 0.8023, 0.8356],\n","         [0.0610, 0.8565, 0.6519, 0.5688, 0.1756, 0.2084, 0.8504],\n","         [0.0543, 0.9596, 0.9283, 0.9012, 0.2792, 0.5373, 0.3593]]])\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["# Erro Scopido\n","## The \"Contiguous\" Trap\n","\n","If you see the error RuntimeError: view size is not compatible with input tensor's size and stride, it's because you tried to view a tensor after transposing it."],"metadata":{"id":"HROhZJDsuYbB"}},{"cell_type":"markdown","source":["#### **The Fix:**"],"metadata":{"id":"ndwvHaLVuucw"}},{"cell_type":"code","source":["# This might crash\n","# y = img_tensor.transpose(1, 2).view(-1)\n","\n","# This will always work\n","y = img_tensor.transpose(1, 2).contiguous().view(-1)"],"metadata":{"id":"Hc12Vthzus8B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Why?** `transpose` doesn't move data in memory; it just changes how we look at it. `view` requires the data to be physically lined up in order. `contiguous()` forces PyTorch to copy the data into a clean, ordered block."],"metadata":{"id":"hE7hTbinu8KB"}},{"cell_type":"markdown","source":["\n","\n","## ðŸš€ PyTorch Tensor Shape Cheat Sheet\n","\n","| Operation | Syntax | What it does | Real-world Use Case |\n","| --- | --- | --- | --- |\n","| **View** | `x.view(shape)` | Changes shape (must be contiguous). | Flattening features before a Linear layer. |\n","| **Reshape** | `x.reshape(shape)` | Changes shape (copies data if needed). | Same as view, but \"safer\" and slightly slower. |\n","| **Unsqueeze** | `x.unsqueeze(dim)` | Adds a dimension of size 1 at `dim`. | Converting a single image into a \"batch of 1\". |\n","| **Squeeze** | `x.squeeze(dim)` | Removes dimension(s) of size 1. | Removing the batch dimension after inference. |\n","| **Transpose** | `x.transpose(d1, d2)` | Swaps two specific dimensions. | Flipping height and width or matrix math. |\n","| **Permute** | `x.permute(dims)` | Reorders all dimensions at once. | Converting Image format: `(C, H, W)`  `(H, W, C)`. |\n","\n","---\n","\n","###  Pro-Tips for  Coding\n","\n","* **The `-1` Trick:** Use `x.view(batch_size, -1)` to let PyTorch calculate the remaining dimension automatically.\n","* **The Error Fixer:** If you get a memory error after transposing, chain `.contiguous()` before your next operation.\n","* **Debug often:** Use `print(x.shape)` after every transformation to ensure your \"mental model\" matches the code.\n","\n"],"metadata":{"id":"1G0hjhnQvM-V"}},{"cell_type":"markdown","source":["# Pro"],"metadata":{"id":"MhZndYJUwSY8"}},{"cell_type":"markdown","source":["# 1. expand() vs. repeat()"],"metadata":{"id":"lew4nvBXwXUA"}},{"cell_type":"markdown","source":["These are used when you want to make a tensor \"larger\" by copying data.\n","\n","* `expand()`: Does not use extra memory. It creates a new view that looks like the data is repeated. It only works on dimensions of size 1.\n","\n","* `repeat()`: Actually copies the data in memory. It's \"expensive\" compared to expand."],"metadata":{"id":"mYUS4ouewf8x"}},{"cell_type":"code","source":["x = torch.tensor([[1], [2], [3]]) # Shape [3, 1]\n","\n","# Expand to [3, 4] - effectively 0 extra memory\n","expanded = x.expand(3, 4)\n","\n","# Repeat to [3, 4] - physically copies data 4 times\n","repeated = x.repeat(1, 4)"],"metadata":{"id":"_8iypL0TwUrF","executionInfo":{"status":"ok","timestamp":1768415208496,"user_tz":-330,"elapsed":152,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["print(expanded)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O-fMZgOJwsMW","executionInfo":{"status":"ok","timestamp":1768415234116,"user_tz":-330,"elapsed":149,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"04b9e7d4-ad7e-45d7-c070-2c8b02994132"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1, 1, 1, 1],\n","        [2, 2, 2, 2],\n","        [3, 3, 3, 3]])\n"]}]},{"cell_type":"code","source":["print(repeated)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O8B10FtawxCX","executionInfo":{"status":"ok","timestamp":1768415237872,"user_tz":-330,"elapsed":181,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"c8269311-23c1-4fce-8fe4-4ecd78fc1bdb"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1, 1, 1, 1],\n","        [2, 2, 2, 2],\n","        [3, 3, 3, 3]])\n"]}]},{"cell_type":"markdown","source":["# 2. flatten()\n","While `view(batch, -1)` is common, `torch.flatten()` is cleaner when you want to squash a specific range of dimensions."],"metadata":{"id":"mX0SCMrPxE9W"}},{"cell_type":"code","source":["# [Batch, Channel, Height, Width]\n","x = torch.randn(32, 3, 224, 224)\n","\n","# Flatten only the spatial/channel dims, keep batch (dim 0)\n","# Start at dim 1, go to the end\n","flat = torch.flatten(x, start_dim=1)\n","print(flat.shape) # [32, 150528]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-hl1wZ2exIaz","executionInfo":{"status":"ok","timestamp":1768415373417,"user_tz":-330,"elapsed":126,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"9edfd7aa-3e9d-4991-a8e2-3d82d418906d"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 150528])\n"]}]},{"cell_type":"markdown","source":["# 3. stack() vs. cat()\n","This is the #1 source of confusion for beginners.\n","\n","* `cat()` **(Concatenate)**: Joins tensors along an existing dimension. The shape stays the same rank (e.g., 2D stays 2D).\n","\n","* `stack()`: Joins tensors along a new dimension. It increases the rank (e.g., a list of 2D tensors becomes one 3D tensor)."],"metadata":{"id":"4ZISfT0SxUP7"}},{"cell_type":"code","source":["t1 = torch.randn(224, 224)\n","t2 = torch.randn(224, 224)\n","\n","# cat: [448, 224] - Glue them together\n","concatenated = torch.cat([t1, t2], dim=0)\n","\n","# stack: [2, 224, 224] - Put one on top of the other\n","stacked = torch.stack([t1, t2], dim=0)"],"metadata":{"id":"HRm8zzZKxbgL","executionInfo":{"status":"ok","timestamp":1768415415556,"user_tz":-330,"elapsed":204,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}}},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":["# Right Question\n","Before you close your Colab, ask yourself:\n","\n","1. Do I need to move data? (If yes, use `reshape` or `repeat`).\n","\n","2. Is this a \"view\"? (If yes, remember `contiguous()` might be needed later).\n","\n","3. Am I adding a new axis? (Use `unsqueeze` or `stack`)."],"metadata":{"id":"oId39xrwxzLs"}},{"cell_type":"markdown","source":["# Github commit and push"],"metadata":{"id":"4xxx6pFUw6pN"}},{"cell_type":"code","source":["# Push notebook changes to GitHub\n","# IMPORTANT: Press Ctrl+S (Save) before running this!\n","!git add .\n","!git commit -m \"Random Tensor to image\"\n","!git push origin main"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ND4rL8Rw6QO","executionInfo":{"status":"ok","timestamp":1768415983079,"user_tz":-330,"elapsed":2131,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"d4c96827-2762-45d4-afdf-ef61d4e7d206"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["[main 70ac780] Random Tensor to image\n"," 1 file changed, 1 insertion(+), 1 deletion(-)\n","Enumerating objects: 5, done.\n","Counting objects: 100% (5/5), done.\n","Delta compression using up to 2 threads\n","Compressing objects: 100% (3/3), done.\n","Writing objects: 100% (3/3), 6.59 KiB | 482.00 KiB/s, done.\n","Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\n","remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n","To https://github.com/barada02/DL_With_Pytorch.git\n","   715c352..70ac780  main -> main\n"]}]}]}