{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP7fvzqskboQ68ls6wRgzP3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5_zYNoLp5IXr","executionInfo":{"status":"ok","timestamp":1768099059828,"user_tz":-330,"elapsed":72526,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"5e447062-0893-4f50-f01c-0f3f77705ad5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/ML/DL_With_Pytorch\n","remote: Enumerating objects: 5, done.\u001b[K\n","remote: Counting objects: 100% (5/5), done.\u001b[K\n","remote: Compressing objects: 100% (3/3), done.\u001b[K\n","remote: Total 3 (delta 2), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n","Unpacking objects: 100% (3/3), 940 bytes | 1024 bytes/s, done.\n","From https://github.com/barada02/DL_With_Pytorch\n"," * branch            main       -> FETCH_HEAD\n","   2cfe18a..2f17d93  main       -> origin/main\n","Updating 2cfe18a..2f17d93\n","Fast-forward\n"," README.md | 11 \u001b[32m+\u001b[m\u001b[31m----------\u001b[m\n"," 1 file changed, 1 insertion(+), 10 deletions(-)\n","✅ Environment Ready!\n"]}],"source":["# Using Drive as storage and github for version controll.\n","\n","from google.colab import drive, userdata\n","import os\n","\n","# 1. Mount Drive\n","drive.mount('/content/drive')\n","\n","# 2. Setup Paths (Change to your actual repo name)\n","REPO_PATH = \"/content/drive/MyDrive/ML/DL_With_Pytorch\"\n","%cd {REPO_PATH}\n","\n","# 3. Secure Auth\n","token = userdata.get('GH_TOKEN')\n","username = \"barada02\"\n","repo = \"DL_With_Pytorch\"\n","!git remote set-url origin https://{token}@github.com/{username}/{repo}.git\n","\n","# 4. Identity\n","!git config --global user.email \"Chandanbarada2@gmail.com\"\n","!git config --global user.name \"Kumar\"\n","\n","!git pull origin main\n","print(\"✅ Environment Ready!\")"]},{"cell_type":"markdown","source":["### Commit and push"],"metadata":{"id":"YyU-kiMrJMQr"}},{"cell_type":"code","source":["# Push notebook changes to GitHub\n","# IMPORTANT: Press Ctrl+S (Save) before running this!\n","!git add .\n","!git commit -m \"Single-element tensors \"\n","!git push origin main"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XE9WW_YF6HqA","executionInfo":{"status":"ok","timestamp":1768106419283,"user_tz":-330,"elapsed":2132,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"710b80e0-2490-4e14-dd86-ffc34b564b15"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["[main b77d96c] Broadcasting\n"," 1 file changed, 1 insertion(+), 1 deletion(-)\n"," rewrite 01_Tensors_02_operations.ipynb (87%)\n","Enumerating objects: 5, done.\n","Counting objects: 100% (5/5), done.\n","Delta compression using up to 2 threads\n","Compressing objects: 100% (3/3), done.\n","Writing objects: 100% (3/3), 1.14 KiB | 117.00 KiB/s, done.\n","Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\n","remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n","To https://github.com/barada02/DL_With_Pytorch.git\n","   325729a..b77d96c  main -> main\n"]}]},{"cell_type":"markdown","source":["# Note book starts from here"],"metadata":{"id":"6z2VJMPh6miG"}},{"cell_type":"code","source":["\n","import torch\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","print(torch.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_lCavztT6x0b","executionInfo":{"status":"ok","timestamp":1768099461794,"user_tz":-330,"elapsed":3467,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"aacfbed8-8404-4b81-fef1-1f1808e8b939"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["2.9.0+cpu\n"]}]},{"cell_type":"markdown","source":["# Tensor operations"],"metadata":{"id":"1T-SSLpx6szV"}},{"cell_type":"markdown","source":["* By default, tensors are created on the CPU. We need to explicitly move tensors to the accelerator using ```.to``` method (after checking for accelerator availability).\n","* Keep in mind that copying large tensors across devices can be expensive in terms of time and memory!"],"metadata":{"id":"4GlWLw-17zze"}},{"cell_type":"code","source":["# We move our tensor to the current accelerator if available\n","if torch.accelerator.is_available():\n","    tensor = tensor.to(torch.accelerator.current_accelerator())"],"metadata":{"id":"IN2nLxAk6sM3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Standard numpy-like indexing and slicking:"],"metadata":{"id":"vGMA7GUG8nhV"}},{"cell_type":"code","source":["tensor = torch.ones(3,3)\n","print(tensor)\n","print(tensor.dtype)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FkhezX998CnR","executionInfo":{"status":"ok","timestamp":1768099683681,"user_tz":-330,"elapsed":212,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"b94291e9-ca48-4cf8-d7e3-8c802bf15d3a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.]])\n","torch.float32\n"]}]},{"cell_type":"code","source":["print(f\"First row:{tensor[0]}\")\n","print(f\"First coloumn:{tensor[:,0]}\")\n","print(f\"Last coloumn:{tensor[...,-1]}\")\n","tensor[:,1]=0 #slicing and assignment.In-place Operation: This modifies the original tensor. It doesn't create a new one;\n","print(tensor)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pXvkjsV_84GR","executionInfo":{"status":"ok","timestamp":1768099789970,"user_tz":-330,"elapsed":248,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"9c4d2ae5-3f82-490a-fcf8-50552c52f3ff"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["First row:tensor([1., 1., 1.])\n","First coloumn:tensor([1., 1., 1.])\n","Last coloumn:tensor([1., 1., 1.])\n","tensor([[1., 0., 1.],\n","        [1., 0., 1.],\n","        [1., 0., 1.]])\n"]}]},{"cell_type":"markdown","source":["## Joining Tensors\n","You can use ```torch.cat``` to concatenate a sequence of tensors along a given dimension. See also ```torch.stack```, another tensor joining operator that is subtly different from ```torch.cat```"],"metadata":{"id":"yHw0-WUW9lmi"}},{"cell_type":"code","source":["# When dim=0, the columns must be identical.\n","\n","# t1 is 2 rows, 3 columns\n","t1 = torch.randn(2, 3)\n","# t2 is 4 rows, 3 columns\n","t2 = torch.full((4, 3), 2.0)\n","\n","# They match in the second dimension (3), so we can stack them vertically\n","vertical_stack = torch.cat([t1, t2], dim=0)\n","\n","print(f\"Tensor 1 Shape: {t1.shape}\")\n","print(f\"Tensor 2 Shape: {t2.shape}\")\n","print(f\"Result Shape:   {vertical_stack.shape}\")\n","print(\"\\nResulting Tensor:\\n\", vertical_stack)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PIcSPV539lHS","executionInfo":{"status":"ok","timestamp":1768103058704,"user_tz":-330,"elapsed":15,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"66d0ae6e-97b9-46f0-804c-01a280ad99fa"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor 1 Shape: torch.Size([2, 3])\n","Tensor 2 Shape: torch.Size([4, 3])\n","Result Shape:   torch.Size([6, 3])\n","\n","Resulting Tensor:\n"," tensor([[ 2.6525,  0.0903,  0.6860],\n","        [-1.0292,  0.6796, -0.2612],\n","        [ 2.0000,  2.0000,  2.0000],\n","        [ 2.0000,  2.0000,  2.0000],\n","        [ 2.0000,  2.0000,  2.0000],\n","        [ 2.0000,  2.0000,  2.0000]])\n"]}]},{"cell_type":"code","source":["# When dim=1, the rows must be identical.\n","\n","# t1 is 2 rows, 3 columns\n","# t1 = torch.rand(2, 3)\n","# t3 is 2 rows, 5 columns\n","t3 = torch.full((2, 5), 3.0)\n","\n","# They match in the first dimension (2), so we can glue them side-by-side\n","horizontal_stack = torch.cat([t1, t3], dim=1)\n","\n","print(f\"Tensor 1 Shape: {t1.shape}\")\n","print(f\"Tensor 3 Shape: {t3.shape}\")\n","print(f\"Result Shape:   {horizontal_stack.shape}\")\n","print(\"\\nResulting Tensor:\\n\", horizontal_stack)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kd6sHRzsDPrk","executionInfo":{"status":"ok","timestamp":1768103061170,"user_tz":-330,"elapsed":20,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"5acb2b21-29f9-4467-a23b-86113eefb6cb"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor 1 Shape: torch.Size([2, 3])\n","Tensor 3 Shape: torch.Size([2, 5])\n","Result Shape:   torch.Size([2, 8])\n","\n","Resulting Tensor:\n"," tensor([[ 2.6525,  0.0903,  0.6860,  3.0000,  3.0000,  3.0000,  3.0000,  3.0000],\n","        [-1.0292,  0.6796, -0.2612,  3.0000,  3.0000,  3.0000,  3.0000,  3.0000]])\n"]}]},{"cell_type":"code","source":["t4 = torch.randn(2,2,2)\n","print(t4)\n","t5 = torch.randn(2,2,2)\n","print(t5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"boBZAWP5DUa4","executionInfo":{"status":"ok","timestamp":1768101406536,"user_tz":-330,"elapsed":238,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"352b47fe-8c39-4261-81d3-a29938565f63"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[-1.0604,  0.7685],\n","         [ 0.8733,  0.4160]],\n","\n","        [[-1.9874,  0.3755],\n","         [ 0.0278,  0.5052]]])\n","tensor([[[ 0.1598, -1.0132],\n","         [-0.1319,  0.2742]],\n","\n","        [[ 0.6721, -0.5150],\n","         [-0.2022, -1.4717]]])\n"]}]},{"cell_type":"code","source":["torch.cat([t4,t5],dim=0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-XVZVAxwDnv0","executionInfo":{"status":"ok","timestamp":1768101426290,"user_tz":-330,"elapsed":239,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"71fa6edd-1333-4258-a5f6-1038d5bcce35"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[-1.0604,  0.7685],\n","         [ 0.8733,  0.4160]],\n","\n","        [[-1.9874,  0.3755],\n","         [ 0.0278,  0.5052]],\n","\n","        [[ 0.1598, -1.0132],\n","         [-0.1319,  0.2742]],\n","\n","        [[ 0.6721, -0.5150],\n","         [-0.2022, -1.4717]]])"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["torch.cat([t4,t5],dim=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7dGVMK7rDtnd","executionInfo":{"status":"ok","timestamp":1768101447956,"user_tz":-330,"elapsed":202,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"4b691283-24d8-4b12-d224-62445292fea8"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[-1.0604,  0.7685],\n","         [ 0.8733,  0.4160],\n","         [ 0.1598, -1.0132],\n","         [-0.1319,  0.2742]],\n","\n","        [[-1.9874,  0.3755],\n","         [ 0.0278,  0.5052],\n","         [ 0.6721, -0.5150],\n","         [-0.2022, -1.4717]]])"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["torch.cat([t4,t5],dim=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HadvROQADxc9","executionInfo":{"status":"ok","timestamp":1768101467224,"user_tz":-330,"elapsed":242,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"c61094a8-2af4-4d37-cdcb-673470296a89"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[-1.0604,  0.7685,  0.1598, -1.0132],\n","         [ 0.8733,  0.4160, -0.1319,  0.2742]],\n","\n","        [[-1.9874,  0.3755,  0.6721, -0.5150],\n","         [ 0.0278,  0.5052, -0.2022, -1.4717]]])"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","source":["\n","\n","> Concatenates the given sequence of tensors in tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be a 1-D empty tensor with size (0,).\n","\n",">```torch.cat()``` can be seen as an inverse operation for ```torch.split()``` and ```torch.chunk()```.\n","\n",">```torch.cat()``` can be best understood via examples.\n","\n"],"metadata":{"id":"pk5GsaeXEth6"}},{"cell_type":"markdown","source":["#### mismatch"],"metadata":{"id":"6rXb41uQKKd3"}},{"cell_type":"code","source":["t_a = torch.randn(2, 3) # Width 3\n","t_b = torch.randn(2, 4) # Width 4\n","\n","print(\"Attempting to concat (2,3) and (2,4) on dim=0...\")\n","try:\n","    # This will fail because you can't stack a width of 3 on a width of 4\n","    error_stack = torch.cat([t_a, t_b], dim=0)\n","except RuntimeError as e:\n","    print(f\"\\nCaught Expected Error: \\n{e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wUdsxwmyKEee","executionInfo":{"status":"ok","timestamp":1768103115585,"user_tz":-330,"elapsed":46,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"7b0d89dc-29ed-44c5-eea9-8f8a42f5cc9d"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Attempting to concat (2,3) and (2,4) on dim=0...\n","\n","Caught Expected Error: \n","Sizes of tensors must match except in dimension 0. Expected size 3 but got size 4 for tensor number 1 in the list.\n"]}]},{"cell_type":"markdown","source":["* The 1-D Empty Tensor Exception\n","* This is how you \"grow\" a tensor starting from nothing."],"metadata":{"id":"7sEdMtemKi74"}},{"cell_type":"code","source":["# A 1-D empty tensor\n","collector = torch.tensor([])\n","\n","# Data to add\n","item1 = torch.tensor([10, 20])\n","item2 = torch.tensor([30, 40, 50])\n","\n","# First concat: empty + item1\n","collector = torch.cat([collector, item1], dim=0)\n","print(f\"After 1st addition: {collector}\")\n","\n","# Second concat: item1 + item2\n","collector = torch.cat([collector, item2], dim=0)\n","print(f\"After 2nd addition: {collector}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O3FgHP3qKP9p","executionInfo":{"status":"ok","timestamp":1768103181081,"user_tz":-330,"elapsed":24,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"9a1f7357-692f-4292-c594-a4f4f0af60e2"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["After 1st addition: tensor([10., 20.])\n","After 2nd addition: tensor([10., 20., 30., 40., 50.])\n"]}]},{"cell_type":"markdown","source":["#### Higher dimensions (3D)\n","In Deep Learning, we often use 3D (Sequence, Batch, Feature) or 4D (Batch, Channel, Height, Width). The rule stays the same: only the dimension you are joining can be different."],"metadata":{"id":"oOHZte5fKz4d"}},{"cell_type":"code","source":["# Imagine two batches of images\n","# Shape: (Batch, Channel, Height, Width)\n","batch1 = torch.randn(16, 3, 64, 64)\n","batch2 = torch.randn(8, 3, 64, 64)\n","\n","# Concatenate batches together\n","big_batch = torch.cat([batch1, batch2], dim=0)\n","\n","print(f\"Batch 1: {batch1.shape}\")\n","print(f\"Batch 2: {batch2.shape}\")\n","print(f\"Combined Batch: {big_batch.shape}\") # (24, 3, 64, 64)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g373S46LK9v_","executionInfo":{"status":"ok","timestamp":1768103347906,"user_tz":-330,"elapsed":45,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"19589947-78a2-4227-e300-ed4b9d49d2db"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Batch 1: torch.Size([16, 3, 64, 64])\n","Batch 2: torch.Size([8, 3, 64, 64])\n","Combined Batch: torch.Size([24, 3, 64, 64])\n"]}]},{"cell_type":"markdown","source":["#### Difference between ```.cat``` and ```.stack```\n","\n","Think of it like this:\n","\n","* **`.cat` (Concatenate):** Takes existing blocks and makes a **longer/wider** block. **(Dimension count stays the same).**\n","* **`.stack`:** Takes existing blocks and puts them in a **new box**. **(Dimension count increases)**.\n","\n","---\n","\n","### 1. The Core Difference\n","\n","| Feature | `torch.cat` | `torch.stack` |\n","| --- | --- | --- |\n","| **Dimensions** | Stays the same (e.g., 2D remains 2D). | **Increases** by 1 (e.g., 1D becomes 2D). |\n","| **Rule** | Tensors must match in all dims except the one you join. | All tensors must be **exactly** the same shape. |\n","| **Analogy** | Taping two sheets of paper together side-by-side. | Putting two sheets of paper on top of each other to make a book. |\n","\n","---\n","\n","\n","\n","###  Which one should you use?\n","\n","* Use **`.cat`** when you want to append data (like adding more rows to a dataset or **more features to a vector**).\n","* Use **`.stack`** when you have multiple separate samples (like 10 individual images) and you want to turn them into a single **batch**.\n","\n","> **Pro Tip:** Stacking is actually the same as adding a new \"fake\" dimension using `.unsqueeze()` and then using `.cat`.\n","\n","[Stack vs Concat in PyTorch, TensorFlow & NumPy](https://www.youtube.com/watch?v=kF2AlpykJGY)\n","\n"],"metadata":{"id":"1BEAT9nQO2oR"}},{"cell_type":"code","source":["\n","# Notice the shape stays 1D, just longer.\n","\n","import torch\n","t1 = torch.tensor([1, 2, 3])\n","t2 = torch.tensor([4, 5, 6])\n","\n","# Joining along the existing dimension\n","res_cat = torch.cat([t1, t2], dim=0)\n","\n","print(f\"Cat Shape: {res_cat.shape}\") # Result: torch.Size([6])\n","print(f\"dimension: {res_cat.dim()}\")\n","print(f\"Cat Result: {res_cat}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u4s14PIhPxjg","executionInfo":{"status":"ok","timestamp":1768104880500,"user_tz":-330,"elapsed":16,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"bc31a8f1-3c3d-4fc7-ce61-c8d86fdbae14"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Cat Shape: torch.Size([6])\n","dimension: 1\n","Cat Result: tensor([1, 2, 3, 4, 5, 6])\n"]}]},{"cell_type":"markdown","source":["###  `torch.stack`\n","\n"," Notice it creates a **new** dimension (it becomes 2D)."],"metadata":{"id":"LouI2lJyQBWT"}},{"cell_type":"code","source":["\n","# Stacking them creates a \"container\" for both\n","res_stack = torch.stack([t1, t2], dim=0)\n","\n","print(f\"Stack Shape: {res_stack.shape}\") # Result: torch.Size([2, 3])\n","print(f\"dimension: {res_stack.dim()}\")\n","print(\"\")\n","print(f\"Stack Result:\\n{res_stack}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TmmYTGPQP79H","executionInfo":{"status":"ok","timestamp":1768104907208,"user_tz":-330,"elapsed":20,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"5c282957-e6df-4272-8f25-3b28fe56c566"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["Stack Shape: torch.Size([2, 3])\n","dimension: 2\n","\n","Stack Result:\n","tensor([[1, 2, 3],\n","        [4, 5, 6]])\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"yty8h6MPP21R"}},{"cell_type":"markdown","source":["## Arithmetic Operations"],"metadata":{"id":"xD4bIigJBTIl"}},{"cell_type":"markdown","source":["#### Matrix Multiplication"],"metadata":{"id":"SfLuDynwR6i_"}},{"cell_type":"code","source":["tensor = torch.full((2, 2),5)\n","print(tensor)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nnqnxT7SSOJV","executionInfo":{"status":"ok","timestamp":1768105443295,"user_tz":-330,"elapsed":50,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"a1ecdacb-0624-4e15-8908-ce83afb734c4"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[5, 5],\n","        [5, 5]])\n"]}]},{"cell_type":"code","source":["\n","\n","# --- Matrix Multiplication (Dot Product) ---\n","# Results in a 2x2 matrix where each element is (5*5 + 5*5) = 50\n","mat_mu1 = tensor @ tensor.T\n","mat_mu2 = tensor.matmul(tensor.T)\n","\n","mat_mu3 = torch.empty_like(mat_mu1) # Using empty_like is safer/faster\n","torch.matmul(tensor, tensor.T, out=mat_mu3)\n","\n","# --- Element-wise Multiplication (Hadamard Product) ---\n","# Results in a 2x2 matrix where each element is (5*5) = 25\n","el_wise1 = tensor * tensor\n","el_wise2 = tensor.mul(tensor)\n","\n","el_wise3 = torch.empty_like(tensor)\n","torch.mul(tensor, tensor, out=el_wise3)\n","\n","print(\"Matrix Mult Result:\\n\", mat_mu1)\n","print(\"\")\n","print(\"Element-wise Result:\\n\", el_wise1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-tNc-0Oq_ifF","executionInfo":{"status":"ok","timestamp":1768105667596,"user_tz":-330,"elapsed":19,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"d8702860-3fd3-4070-9a06-68300f35168e"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["Matrix Mult Result:\n"," tensor([[50, 50],\n","        [50, 50]])\n","\n","Element-wise Result:\n"," tensor([[25, 25],\n","        [25, 25]])\n"]}]},{"cell_type":"markdown","source":["\n","\n","## 1. Matrix Multiplication (The Dot Product)\n","\n","This operation follows the **row-by-column** rule. In your code, you used `tensor @ tensor.T`. If we represent the original tensor as , then the multiplication  is defined as:\n","\n","**General Definition:**\n","For  and , the element at row  and column  is:\n","\n","\n","---\n","\n","## 2. Element-wise Multiplication (The Hadamard Product)\n","\n","In your code, you used `tensor * tensor`. This is the **Hadamard Product**, denoted by the symbol . It simply multiplies the numbers that are in the same position.\n","\n","**General Definition:**\n","For , the resulting element is:\n","\n","\n","---\n","\n","### Comparison Summary\n","\n","| Property | Matrix Multiplication (`@`) | Element-wise (`*`) |\n","| --- | --- | --- |\n","| **Math Symbol** |  or  |  |\n","| **Pytorch** | `torch.matmul(A, B)` | `torch.mul(A, B)` |\n","| **Requirement** | Columns of  == Rows of  | Shapes must be identical* |\n","| **Operation** | Sum of products (Dot product) | Individual products |\n","\n","> **Note on** `out=y3`: Mathematically, this is just an assignment: . In PyTorch, this is a memory-efficient way to perform the calculation by reusing an existing allocated memory block.\n"],"metadata":{"id":"WtgmO0jFVcIX"}},{"cell_type":"markdown","source":["## 1. Matrix Multiplication (The Dot Product)\n","\n","This operation follows the **row-by-column** rule. In your code, you used `tensor @ tensor.T`. If we represent the original tensor as , then the multiplication  is defined as:\n","\n","**General Definition:**\n","For  and , the element at row  and column  is:\n","\n","\n","---\n","\n","## 2. Element-wise Multiplication (The Hadamard Product)\n","\n","In your code, you used `tensor * tensor`. This is the **Hadamard Product**, denoted by the symbol . It simply multiplies the numbers that are in the same position.\n","\n","**General Definition:**\n","For , the resulting element is:\n","\n","\n","---\n","\n","### Comparison Summary\n","\n","| Property | Matrix Multiplication (`@`) | Element-wise (`*`) |\n","| --- | --- | --- |\n","| **Math Symbol** |  or  |  |\n","| **Pytorch** | `torch.matmul(A, B)` | `torch.mul(A, B)` |\n","| **Requirement** | Columns of  == Rows of  | Shapes must be identical* |\n","| **Operation** | Sum of products (Dot product) | Individual products |\n","\n","> **Note on `out=y3**`: Mathematically, this is just an assignment: . In PyTorch, this is a memory-efficient way to perform the calculation by reusing an existing allocated memory block.\n"],"metadata":{"id":"ngtxc9QrWfDg"}},{"cell_type":"markdown","source":["##Single-element tensors\n","If you have a one-element tensor, for example by aggregating all values of a tensor into one value, you can convert it to a Python numerical value using ```item():```"],"metadata":{"id":"lE9stQMtW6Cq"}},{"cell_type":"code","source":["print(tensor, tensor.dtype)\n","agg = tensor.sum()\n","print(agg, agg.dtype)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7qbxI_3oXFtK","executionInfo":{"status":"ok","timestamp":1768106539054,"user_tz":-330,"elapsed":19,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"1279a5f6-a375-4a55-eab8-cb8bb4d583a3"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[5, 5],\n","        [5, 5]]) torch.int64\n","tensor(20) torch.int64\n"]}]},{"cell_type":"code","source":["agg = tensor.sum()\n","agg_item = agg.item()\n","print(agg_item, type(agg_item))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pi-LoPbvXBAF","executionInfo":{"status":"ok","timestamp":1768106508645,"user_tz":-330,"elapsed":43,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"}},"outputId":"0a2a22ce-9358-4358-f1fb-2de6e71f9b5d"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["20 <class 'int'>\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"zDrxGCVcVeY0"},"execution_count":null,"outputs":[]}]}