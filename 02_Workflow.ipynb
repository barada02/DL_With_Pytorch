{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOPf6zLD-xan"
   },
   "source": [
    "## Github setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48748,
     "status": "ok",
     "timestamp": 1768217912428,
     "user": {
      "displayName": "Chandan Kumar Barada",
      "userId": "01468100662972031370"
     },
     "user_tz": -330
    },
    "id": "uPMeCQLY-cry",
    "outputId": "313b66ac-2fff-4606-f0e2-d087f71ed893"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/MyDrive/ML/DL_With_Pytorch\n",
      "From https://github.com/barada02/DL_With_Pytorch\n",
      " * branch            main       -> FETCH_HEAD\n",
      "Already up to date.\n",
      "âœ… Environment Ready!\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive, userdata\n",
    "import os\n",
    "\n",
    "# 1. Mount Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2. Setup Paths (Change to your actual repo name)\n",
    "REPO_PATH = \"/content/drive/MyDrive/ML/DL_With_Pytorch\"\n",
    "%cd {REPO_PATH}\n",
    "\n",
    "# 3. Secure Auth\n",
    "token = userdata.get('GH_TOKEN')\n",
    "username = \"barada02\"\n",
    "repo = \"DL_With_Pytorch\"\n",
    "!git remote set-url origin https://{token}@github.com/{username}/{repo}.git\n",
    "\n",
    "# 4. Identity\n",
    "!git config --global user.email \"Chandanbarada2@gmail.com\"\n",
    "!git config --global user.name \"Kumar\"\n",
    "\n",
    "!git pull origin main\n",
    "print(\"âœ… Environment Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGZfy6ek-rK1"
   },
   "source": [
    "## Commit and Push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1638,
     "status": "ok",
     "timestamp": 1768218459845,
     "user": {
      "displayName": "Chandan Kumar Barada",
      "userId": "01468100662972031370"
     },
     "user_tz": -330
    },
    "id": "Vu0lb7EE-pCH",
    "outputId": "168e7f31-7621-412a-d1a8-d201d60c5961"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main ed803d8] Optimization loop\n",
      " 1 file changed, 1 insertion(+), 1 deletion(-)\n",
      " rewrite 02_Workflow.ipynb (73%)\n",
      "Enumerating objects: 5, done.\n",
      "Counting objects: 100% (5/5), done.\n",
      "Delta compression using up to 2 threads\n",
      "Compressing objects: 100% (3/3), done.\n",
      "Writing objects: 100% (3/3), 2.53 KiB | 216.00 KiB/s, done.\n",
      "Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
      "To https://github.com/barada02/DL_With_Pytorch.git\n",
      "   0491d13..ed803d8  main -> main\n"
     ]
    }
   ],
   "source": [
    "# 2. Push notebook changes to GitHub\n",
    "# IMPORTANT: Press Ctrl+S (Save) before running this!\n",
    "!git add .\n",
    "!git commit -m \"Optimization loop\"\n",
    "!git push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-2SMLzI-2ro"
   },
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Deep Learning Workflow\n",
    "\n",
    "This notebook demonstrates the **complete end-to-end workflow** for training a neural network in PyTorch. \n",
    "\n",
    "## ðŸ“‹ Workflow Overview:\n",
    "1. **Setup & Import** - Import necessary libraries\n",
    "2. **Data Preparation** - Load and prepare datasets\n",
    "3. **Model Definition** - Create neural network architecture\n",
    "4. **Training Configuration** - Set hyperparameters, loss function, optimizer\n",
    "5. **Training Loop** - Train the model\n",
    "6. **Evaluation** - Test model performance\n",
    "7. **Model Persistence** - Save and load trained models\n",
    "\n",
    "This workflow is fundamental to all deep learning projects and can be adapted for various tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWJIFe5G-5Sg"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "**Purpose**: Import PyTorch and related libraries needed for the deep learning workflow.\n",
    "\n",
    "**Key Imports**:\n",
    "- `torch` - Core PyTorch library\n",
    "- `torch.nn` - Neural network modules and building blocks\n",
    "- `torch.utils.data.DataLoader` - Efficient data loading with batching\n",
    "- `torchvision.datasets` - Pre-built datasets (FashionMNIST in this case)\n",
    "- `torchvision.transforms` - Data preprocessing and augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8273,
     "status": "ok",
     "timestamp": 1768217946843,
     "user": {
      "displayName": "Chandan Kumar Barada",
      "userId": "01468100662972031370"
     },
     "user_tz": -330
    },
    "id": "mKQm-355--P9",
    "outputId": "2d69ac15-4301-4eab-b26b-3fb6c9471378"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5595,
     "status": "ok",
     "timestamp": 1768217981379,
     "user": {
      "displayName": "Chandan Kumar Barada",
      "userId": "01468100662972031370"
     },
     "user_tz": -330
    },
    "id": "qvDHsz3Z_c__",
    "outputId": "eb6fd475-2a16-4e37-acbb-109d8f6ef39e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26.4M/26.4M [00:01<00:00, 16.2MB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29.5k/29.5k [00:00<00:00, 282kB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.42M/4.42M [00:00<00:00, 5.03MB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.15k/5.15k [00:00<00:00, 22.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Prepare Data\n",
    "\n",
    "**Purpose**: Download and prepare the FashionMNIST dataset for training and testing.\n",
    "\n",
    "**What is FashionMNIST?**\n",
    "- 70,000 grayscale images of clothing items (28x28 pixels)\n",
    "- 10 classes: T-shirt, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot\n",
    "- Split into 60,000 training and 10,000 test images\n",
    "\n",
    "**Key Parameters**:\n",
    "- `root=\"data\"` - Directory to store the dataset\n",
    "- `train=True/False` - Specify training or test set\n",
    "- `download=True` - Automatically download if not present\n",
    "- `transform=ToTensor()` - Convert PIL images to PyTorch tensors (0-1 range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1768217985012,
     "user": {
      "displayName": "Chandan Kumar Barada",
      "userId": "01468100662972031370"
     },
     "user_tz": -330
    },
    "id": "7Kk54wRO_hK1"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create DataLoaders\n",
    "\n",
    "**Purpose**: Wrap datasets in DataLoader for efficient batch processing during training.\n",
    "\n",
    "**Why DataLoader?**\n",
    "- Automatically batches data into manageable chunks\n",
    "- Shuffles data between epochs (for training)\n",
    "- Enables parallel data loading with multiple workers\n",
    "- Memory efficient - loads only the current batch\n",
    "\n",
    "**Batch Size = 64**: Each iteration processes 64 images at once, balancing memory usage and training speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1768217989142,
     "user": {
      "displayName": "Chandan Kumar Barada",
      "userId": "01468100662972031370"
     },
     "user_tz": -330
    },
    "id": "CVFtSCO_-8O6"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define the Neural Network\n",
    "\n",
    "**Purpose**: Create the model architecture that will learn to classify images.\n",
    "\n",
    "**Architecture Breakdown**:\n",
    "1. **Input Layer**: 28Ã—28 = 784 pixels (flattened)\n",
    "2. **Hidden Layer 1**: 784 â†’ 512 neurons with ReLU activation\n",
    "3. **Hidden Layer 2**: 512 â†’ 512 neurons with ReLU activation\n",
    "4. **Output Layer**: 512 â†’ 10 neurons (one per class)\n",
    "\n",
    "**Key Components**:\n",
    "- `nn.Flatten()` - Converts 2D image (28Ã—28) to 1D vector (784)\n",
    "- `nn.Linear()` - Fully connected layers (neurons)\n",
    "- `nn.ReLU()` - Activation function (introduces non-linearity)\n",
    "- `nn.Sequential()` - Chains layers together\n",
    "\n",
    "**Forward Pass**: Defines how data flows through the network from input to output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 72,
     "status": "ok",
     "timestamp": 1768217998500,
     "user": {
      "displayName": "Chandan Kumar Barada",
      "userId": "01468100662972031370"
     },
     "user_tz": -330
    },
    "id": "q-GWAPUm_jEG",
    "outputId": "ea70d011-a1d3-402a-cc70-cc1e186b68af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = NeuralNetwork()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Instantiate the Model\n",
    "\n",
    "**Purpose**: Create an instance of the neural network.\n",
    "\n",
    "This creates an untrained model with randomly initialized weights. The model is now ready to be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evsCpar3AoAQ"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuQ6wKRnAtVd"
   },
   "source": [
    "Hyperparameters are adjustable parameters that let you control the model optimization process. Different hyperparameter values can impact model training and convergence rates [read more](https://docs.pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html)\n",
    "\n",
    "We define the following hyperparameters for training:\n",
    "* Number of Epochs - the number of times to iterate over the dataset\n",
    "\n",
    "* Batch Size - the number of data samples propagated through the network before the parameters are updated\n",
    "\n",
    "* Learning Rate - how much to update models parameters at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1768218177626,
     "user": {
      "displayName": "Chandan Kumar Barada",
      "userId": "01468100662972031370"
     },
     "user_tz": -330
    },
    "id": "czw_KuqKAhk2"
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Set Hyperparameters\n",
    "\n",
    "**Purpose**: Configure the training process parameters.\n",
    "\n",
    "**Hyperparameter Definitions**:\n",
    "- **Learning Rate (1e-3 = 0.001)**: Controls how much to adjust weights after each batch\n",
    "  - Too high â†’ unstable training\n",
    "  - Too low â†’ slow learning\n",
    "  \n",
    "- **Batch Size (64)**: Number of samples processed before updating weights\n",
    "  - Larger â†’ more stable gradients, more memory\n",
    "  - Smaller â†’ faster updates, noisier gradients\n",
    "  \n",
    "- **Epochs (5)**: Number of times to iterate through the entire dataset\n",
    "  - More epochs â†’ better learning (but risk overfitting)\n",
    "\n",
    "These are the \"knobs\" you tune to improve model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKFoWCsXBFUj"
   },
   "source": [
    "## Optimization Loop\n",
    "Once we set our hyperparameters, we can then train and optimize our model with an optimization loop. Each iteration of the optimization loop is called an epoch.\n",
    "\n",
    "**Each epoch consists of two main parts:**\n",
    "**The Train Loop** - iterate over the training dataset and try to converge to optimal parameters.\n",
    "\n",
    "**The Validation/Test Loop** - iterate over the test dataset to check if model performance is improving.\n",
    "\n",
    "Letâ€™s briefly familiarize ourselves with some of the concepts used in the training loop. Jump ahead to see the Full Implementation of the optimization loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7FvMr4VBZUC"
   },
   "source": [
    "## Loss Function\n",
    "When presented with some training data, our untrained network is likely not to give the correct answer. Loss function measures the degree of dissimilarity of obtained result to the target value, and it is the loss function that we want to minimize during training. To calculate the loss we make a prediction using the inputs of our given data sample and compare it against the true data label value.\n",
    "\n",
    "Common loss functions include nn.MSELoss (Mean Square Error) for regression tasks, and nn.NLLLoss (Negative Log Likelihood) for classification. nn.CrossEntropyLoss combines `nn.LogSoftmax` and `nn.NLLLoss`.\n",
    "\n",
    "We pass our modelâ€™s output logits to `nn.CrossEntropyLoss`, which will normalize the logits and compute the prediction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1768218337515,
     "user": {
      "displayName": "Chandan Kumar Barada",
      "userId": "01468100662972031370"
     },
     "user_tz": -330
    },
    "id": "IFr343xiBnip"
   },
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Initialize Loss Function\n",
    "\n",
    "**Purpose**: Measure how far the model's predictions are from actual labels.\n",
    "\n",
    "**CrossEntropyLoss**:\n",
    "- Best for multi-class classification (choosing 1 class from many)\n",
    "- Combines LogSoftmax + NLLLoss\n",
    "- Lower loss = better predictions\n",
    "\n",
    "**How it works**: \n",
    "- Compares predicted probabilities with true labels\n",
    "- Penalizes confident wrong predictions more heavily\n",
    "- Goal: Minimize this loss during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KygovxGyBq7w"
   },
   "source": [
    "Optimizer\n",
    "Optimization is the process of adjusting model parameters to reduce model error in each training step. Optimization algorithms define how this process is performed (in this example we use Stochastic Gradient Descent). All optimization logic is encapsulated in the optimizer object. Here, we use the SGD optimizer; additionally, there are many different optimizers available in PyTorch such as ADAM and RMSProp, that work better for different kinds of models and data.\n",
    "\n",
    "We initialize the optimizer by registering the modelâ€™s parameters that need to be trained, and passing in the learning rate hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1768218373241,
     "user": {
      "displayName": "Chandan Kumar Barada",
      "userId": "01468100662972031370"
     },
     "user_tz": -330
    },
    "id": "hsnt4ylBBxIg"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Initialize Optimizer\n",
    "\n",
    "**Purpose**: Algorithm that updates model weights to minimize loss.\n",
    "\n",
    "**SGD (Stochastic Gradient Descent)**:\n",
    "- Classic optimization algorithm\n",
    "- Updates weights based on gradients (derivatives of loss)\n",
    "- \"Descends\" toward minimum loss\n",
    "\n",
    "**Alternative Optimizers**:\n",
    "- `Adam` - Adaptive learning rate, often faster convergence\n",
    "- `RMSProp` - Good for recurrent networks\n",
    "- `AdaGrad` - Adapts learning rate per parameter\n",
    "\n",
    "The optimizer needs:\n",
    "1. Model parameters to update\n",
    "2. Learning rate to control update size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xo1JnaBLBzjQ"
   },
   "source": [
    "Inside the training loop, optimization happens in three steps:\n",
    "Call optimizer.zero_grad() to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
    "\n",
    "Backpropagate the prediction loss with a call to loss.backward(). PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
    "\n",
    "Once we have our gradients, we call optimizer.step() to adjust the parameters by the gradients collected in the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Three Steps of Optimization (per batch):\n",
    "\n",
    "**Visual Flow**: \n",
    "```\n",
    "Data Batch â†’ Forward Pass â†’ Calculate Loss â†’ Backward Pass â†’ Update Weights â†’ Repeat\n",
    "```\n",
    "\n",
    "1. **`optimizer.zero_grad()`** \n",
    "   - Clears old gradients from previous batch\n",
    "   - Prevents gradient accumulation\n",
    "\n",
    "2. **`loss.backward()`** \n",
    "   - Backpropagation: Calculates gradients for all parameters\n",
    "   - Determines how to adjust each weight\n",
    "\n",
    "3. **`optimizer.step()`** \n",
    "   - Updates weights using calculated gradients\n",
    "   - Moves model toward better predictions\n",
    "\n",
    "**Remember**: Zero gradients â†’ Calculate gradients â†’ Apply gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAkjHsWeCLdh"
   },
   "source": [
    "# Full Implementaion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_vJ64HvB7ju"
   },
   "source": [
    "We define train_loop that loops over our optimization code, and test_loop that evaluates the modelâ€™s performance against our test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Define Training and Testing Functions\n",
    "\n",
    "**Purpose**: Encapsulate the training and evaluation logic.\n",
    "\n",
    "### `train_loop()` Function:\n",
    "**What it does**: Trains the model for one epoch\n",
    "- Loops through all training batches\n",
    "- For each batch:\n",
    "  1. Forward pass (get predictions)\n",
    "  2. Calculate loss\n",
    "  3. Backward pass (compute gradients)\n",
    "  4. Update weights\n",
    "  5. Zero gradients for next iteration\n",
    "- Prints loss every 100 batches to monitor progress\n",
    "\n",
    "**Key setting**: `model.train()` - Enables training mode (activates dropout, batch norm)\n",
    "\n",
    "---\n",
    "\n",
    "### `test_loop()` Function:\n",
    "**What it does**: Evaluates model performance on test data\n",
    "- Loops through all test batches\n",
    "- Calculates predictions without updating weights\n",
    "- Computes accuracy and average loss\n",
    "- Reports overall performance\n",
    "\n",
    "**Key settings**: \n",
    "- `model.eval()` - Evaluation mode (disables dropout)\n",
    "- `torch.no_grad()` - Disables gradient computation (saves memory/time)\n",
    "\n",
    "**Why separate test loop?**\n",
    "- Prevents overfitting (model hasn't seen test data)\n",
    "- Validates that model generalizes to new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1768218497176,
     "user": {
      "displayName": "Chandan Kumar Barada",
      "userId": "01468100662972031370"
     },
     "user_tz": -330
    },
    "id": "BUG1V08sB5_R"
   },
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cpssStVCTWh"
   },
   "source": [
    "We initialize the loss function and optimizer, and pass it to train_loop and test_loop. Feel free to increase the number of epochs to track the modelâ€™s improving performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Train the Model\n",
    "\n",
    "**Purpose**: Execute the complete training process.\n",
    "\n",
    "**Training Process** (10 epochs):\n",
    "- **Epoch** = One complete pass through training data\n",
    "- For each epoch:\n",
    "  1. Train on entire training set (`train_loop`)\n",
    "  2. Evaluate on entire test set (`test_loop`)\n",
    "  3. Print progress\n",
    "\n",
    "**What to Watch**:\n",
    "- **Training Loss**: Should decrease over epochs\n",
    "- **Test Accuracy**: Should increase over epochs\n",
    "- **Gap between train and test**: If test accuracy plateaus while training improves, model may be overfitting\n",
    "\n",
    "**Expected Behavior**: \n",
    "- Initial accuracy ~10% (random guessing among 10 classes)\n",
    "- Final accuracy ~85-88% after 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 136668,
     "status": "ok",
     "timestamp": 1768218705696,
     "user": {
      "displayName": "Chandan Kumar Barada",
      "userId": "01468100662972031370"
     },
     "user_tz": -330
    },
    "id": "2PvjfTdzCRqH",
    "outputId": "337e96b7-e992-4c67-c909-76f9f9691864"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.306856  [   64/60000]\n",
      "loss: 2.298582  [ 6464/60000]\n",
      "loss: 2.283126  [12864/60000]\n",
      "loss: 2.264119  [19264/60000]\n",
      "loss: 2.248101  [25664/60000]\n",
      "loss: 2.226551  [32064/60000]\n",
      "loss: 2.225410  [38464/60000]\n",
      "loss: 2.193868  [44864/60000]\n",
      "loss: 2.198018  [51264/60000]\n",
      "loss: 2.159197  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 43.6%, Avg loss: 2.157352 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.168603  [   64/60000]\n",
      "loss: 2.159842  [ 6464/60000]\n",
      "loss: 2.107098  [12864/60000]\n",
      "loss: 2.107127  [19264/60000]\n",
      "loss: 2.065855  [25664/60000]\n",
      "loss: 2.007953  [32064/60000]\n",
      "loss: 2.022113  [38464/60000]\n",
      "loss: 1.944845  [44864/60000]\n",
      "loss: 1.959247  [51264/60000]\n",
      "loss: 1.875185  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 1.881718 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.915567  [   64/60000]\n",
      "loss: 1.884475  [ 6464/60000]\n",
      "loss: 1.778140  [12864/60000]\n",
      "loss: 1.798797  [19264/60000]\n",
      "loss: 1.709102  [25664/60000]\n",
      "loss: 1.660662  [32064/60000]\n",
      "loss: 1.665707  [38464/60000]\n",
      "loss: 1.577755  [44864/60000]\n",
      "loss: 1.611143  [51264/60000]\n",
      "loss: 1.493752  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Avg loss: 1.521906 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.588650  [   64/60000]\n",
      "loss: 1.552963  [ 6464/60000]\n",
      "loss: 1.417349  [12864/60000]\n",
      "loss: 1.466972  [19264/60000]\n",
      "loss: 1.369297  [25664/60000]\n",
      "loss: 1.363701  [32064/60000]\n",
      "loss: 1.360835  [38464/60000]\n",
      "loss: 1.297623  [44864/60000]\n",
      "loss: 1.338738  [51264/60000]\n",
      "loss: 1.230714  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 1.261174 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.334185  [   64/60000]\n",
      "loss: 1.316130  [ 6464/60000]\n",
      "loss: 1.162555  [12864/60000]\n",
      "loss: 1.248604  [19264/60000]\n",
      "loss: 1.139457  [25664/60000]\n",
      "loss: 1.163463  [32064/60000]\n",
      "loss: 1.169330  [38464/60000]\n",
      "loss: 1.118035  [44864/60000]\n",
      "loss: 1.162610  [51264/60000]\n",
      "loss: 1.072293  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 1.094573 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.159976  [   64/60000]\n",
      "loss: 1.162498  [ 6464/60000]\n",
      "loss: 0.990891  [12864/60000]\n",
      "loss: 1.108602  [19264/60000]\n",
      "loss: 0.995759  [25664/60000]\n",
      "loss: 1.026743  [32064/60000]\n",
      "loss: 1.049318  [38464/60000]\n",
      "loss: 1.001662  [44864/60000]\n",
      "loss: 1.045579  [51264/60000]\n",
      "loss: 0.971820  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.985532 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.037144  [   64/60000]\n",
      "loss: 1.061795  [ 6464/60000]\n",
      "loss: 0.872420  [12864/60000]\n",
      "loss: 1.013977  [19264/60000]\n",
      "loss: 0.904201  [25664/60000]\n",
      "loss: 0.929672  [32064/60000]\n",
      "loss: 0.970969  [38464/60000]\n",
      "loss: 0.925437  [44864/60000]\n",
      "loss: 0.964134  [51264/60000]\n",
      "loss: 0.904229  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.910909 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.946680  [   64/60000]\n",
      "loss: 0.992131  [ 6464/60000]\n",
      "loss: 0.787279  [12864/60000]\n",
      "loss: 0.946767  [19264/60000]\n",
      "loss: 0.843376  [25664/60000]\n",
      "loss: 0.858648  [32064/60000]\n",
      "loss: 0.916422  [38464/60000]\n",
      "loss: 0.874524  [44864/60000]\n",
      "loss: 0.905692  [51264/60000]\n",
      "loss: 0.855784  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.4%, Avg loss: 0.857500 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.877950  [   64/60000]\n",
      "loss: 0.940641  [ 6464/60000]\n",
      "loss: 0.724049  [12864/60000]\n",
      "loss: 0.896977  [19264/60000]\n",
      "loss: 0.800490  [25664/60000]\n",
      "loss: 0.805714  [32064/60000]\n",
      "loss: 0.875859  [38464/60000]\n",
      "loss: 0.839527  [44864/60000]\n",
      "loss: 0.862727  [51264/60000]\n",
      "loss: 0.819244  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.817508 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.823851  [   64/60000]\n",
      "loss: 0.900259  [ 6464/60000]\n",
      "loss: 0.675536  [12864/60000]\n",
      "loss: 0.859106  [19264/60000]\n",
      "loss: 0.768456  [25664/60000]\n",
      "loss: 0.765385  [32064/60000]\n",
      "loss: 0.843805  [38464/60000]\n",
      "loss: 0.814111  [44864/60000]\n",
      "loss: 0.830032  [51264/60000]\n",
      "loss: 0.790279  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.786188 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qn__0QgYCxsl"
   },
   "source": [
    "# Save and Load the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Model Persistence\n",
    "\n",
    "**Purpose**: Save trained model for later use without retraining.\n",
    "\n",
    "### Saving the Model:\n",
    "`torch.save(model, 'model.pth')`\n",
    "- Saves entire model (architecture + weights)\n",
    "- `.pth` or `.pt` are common PyTorch model file extensions\n",
    "\n",
    "**Alternative**: `torch.save(model.state_dict(), 'model.pth')`\n",
    "- Saves only weights (smaller file, more flexible)\n",
    "- Requires model architecture definition when loading\n",
    "\n",
    "---\n",
    "\n",
    "### Loading the Model:\n",
    "`torch.load('model.pth', weights_only=False)`\n",
    "- Loads entire saved model\n",
    "- `weights_only=False` - Loads full model (not just weights)\n",
    "- Model is ready to use for predictions immediately\n",
    "\n",
    "**Why Save Models?**\n",
    "- Avoid retraining (saves time and compute)\n",
    "- Deploy models to production\n",
    "- Share models with others\n",
    "- Resume training later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1768219026491,
     "user": {
      "displayName": "Chandan Kumar Barada",
      "userId": "01468100662972031370"
     },
     "user_tz": -330
    },
    "id": "Klxo5umEDs-6"
   },
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth') # or model.state_dict()?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1768219093372,
     "user": {
      "displayName": "Chandan Kumar Barada",
      "userId": "01468100662972031370"
     },
     "user_tz": -330
    },
    "id": "v0bN1IPVDyfh",
    "outputId": "ddf5a6e1-f631-4fcf-e712-a78bfa08baa5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('model.pth', weights_only=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ“ Summary: Complete PyTorch Workflow\n",
    "\n",
    "### The 11-Step Process:\n",
    "\n",
    "1. âœ… **Import** libraries\n",
    "2. âœ… **Load** datasets\n",
    "3. âœ… **Create** DataLoaders\n",
    "4. âœ… **Define** model architecture\n",
    "5. âœ… **Instantiate** model\n",
    "6. âœ… **Set** hyperparameters\n",
    "7. âœ… **Initialize** loss function\n",
    "8. âœ… **Initialize** optimizer\n",
    "9. âœ… **Define** train/test loops\n",
    "10. âœ… **Train** the model\n",
    "11. âœ… **Save/Load** model\n",
    "\n",
    "### Key Concepts to Remember:\n",
    "\n",
    "- **Epochs**: Full passes through the dataset\n",
    "- **Batches**: Subsets of data processed together\n",
    "- **Loss**: Measure of prediction error (lower is better)\n",
    "- **Optimizer**: Updates weights to minimize loss\n",
    "- **Forward Pass**: Data â†’ Model â†’ Predictions\n",
    "- **Backward Pass**: Loss â†’ Gradients â†’ Weight Updates\n",
    "- **Train/Eval Modes**: Different behaviors for training vs testing\n",
    "\n",
    "### This workflow applies to most deep learning problems!\n",
    "Just swap out the dataset, model architecture, and hyperparameters for your specific task.\n",
    "\n",
    "---\n",
    "**Next Steps**: Experiment with different architectures, optimizers, or datasets to deepen understanding!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM299roL2FU5z2gBuRsisM6",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
