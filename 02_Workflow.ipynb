{"cells":[{"cell_type":"markdown","metadata":{"id":"rOPf6zLD-xan"},"source":["## Github setup"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35127,"status":"ok","timestamp":1768330764890,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"},"user_tz":-330},"id":"uPMeCQLY-cry","outputId":"8e5ee8f9-a14d-4e0e-ba36-26250465438c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/ML/DL_With_Pytorch\n","From https://github.com/barada02/DL_With_Pytorch\n"," * branch            main       -> FETCH_HEAD\n","Already up to date.\n","âœ… Environment Ready!\n"]}],"source":["from google.colab import drive, userdata\n","import os\n","\n","# 1. Mount Drive\n","drive.mount('/content/drive')\n","\n","# 2. Setup Paths (Change to your actual repo name)\n","REPO_PATH = \"/content/drive/MyDrive/ML/DL_With_Pytorch\"\n","%cd {REPO_PATH}\n","\n","# 3. Secure Auth\n","token = userdata.get('GH_TOKEN')\n","username = \"barada02\"\n","repo = \"DL_With_Pytorch\"\n","!git remote set-url origin https://{token}@github.com/{username}/{repo}.git\n","\n","# 4. Identity\n","!git config --global user.email \"Chandanbarada2@gmail.com\"\n","!git config --global user.name \"Kumar\"\n","\n","!git pull origin main\n","print(\"âœ… Environment Ready!\")"]},{"cell_type":"markdown","metadata":{"id":"lGZfy6ek-rK1"},"source":["## Commit and Push"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10727,"status":"ok","timestamp":1768330873568,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"},"user_tz":-330},"id":"Vu0lb7EE-pCH","outputId":"51597891-f05c-4d21-c91c-0186e54c453e"},"outputs":[{"output_type":"stream","name":"stdout","text":["[main 6adb7fb] autograd operation diagram\n"," 1 file changed, 1 insertion(+), 1578 deletions(-)\n"," rewrite 02_Workflow.ipynb (99%)\n","Enumerating objects: 5, done.\n","Counting objects: 100% (5/5), done.\n","Delta compression using up to 2 threads\n","Compressing objects: 100% (3/3), done.\n","Writing objects: 100% (3/3), 5.56 KiB | 407.00 KiB/s, done.\n","Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\n","remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n","To https://github.com/barada02/DL_With_Pytorch.git\n","   eaac25e..6adb7fb  main -> main\n"]}],"source":["# 2. Push notebook changes to GitHub\n","# IMPORTANT: Press Ctrl+S (Save) before running this!\n","!git add .\n","!git commit -m \"autograd operation diagram \"\n","!git push origin main"]},{"cell_type":"markdown","metadata":{"id":"Y-2SMLzI-2ro"},"source":["# Workflow"]},{"cell_type":"markdown","metadata":{"id":"tnSARP8HaLtA"},"source":["# PyTorch Deep Learning Workflow\n","\n","This notebook demonstrates the **complete end-to-end workflow** for training a neural network in PyTorch.\n","\n","## ğŸ“‹ Workflow Overview:\n","1. **Setup & Import** - Import necessary libraries\n","2. **Data Preparation** - Load and prepare datasets\n","3. **Model Definition** - Create neural network architecture\n","4. **Training Configuration** - Set hyperparameters, loss function, optimizer\n","5. **Training Loop** - Train the model\n","6. **Evaluation** - Test model performance\n","7. **Model Persistence** - Save and load trained models\n","\n","This workflow is fundamental to all deep learning projects and can be adapted for various tasks."]},{"cell_type":"markdown","metadata":{"id":"ZWJIFe5G-5Sg"},"source":[]},{"cell_type":"markdown","metadata":{"id":"JEkAppSHaLtD"},"source":["## Step 1: Import Required Libraries\n","\n","**Purpose**: Import PyTorch and related libraries needed for the deep learning workflow.\n","\n","**Key Imports**:\n","- `torch` - Core PyTorch library\n","- `torch.nn` - Neural network modules and building blocks\n","- `torch.utils.data.DataLoader` - Efficient data loading with batching\n","- `torchvision.datasets` - Pre-built datasets (FashionMNIST in this case)\n","- `torchvision.transforms` - Data preprocessing and augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8273,"status":"ok","timestamp":1768217946843,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"},"user_tz":-330},"id":"mKQm-355--P9","outputId":"2d69ac15-4301-4eab-b26b-3fb6c9471378"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.9.0+cpu\n"]}],"source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","print(torch.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5595,"status":"ok","timestamp":1768217981379,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"},"user_tz":-330},"id":"qvDHsz3Z_c__","outputId":"eb6fd475-2a16-4e37-acbb-109d8f6ef39e"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26.4M/26.4M [00:01<00:00, 16.2MB/s]\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29.5k/29.5k [00:00<00:00, 282kB/s]\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.42M/4.42M [00:00<00:00, 5.03MB/s]\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.15k/5.15k [00:00<00:00, 22.5MB/s]\n"]}],"source":["\n","training_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform=ToTensor()\n",")\n","\n","test_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform=ToTensor()\n",")"]},{"cell_type":"markdown","metadata":{"id":"Hvdhm6s1aLtG"},"source":["## Step 2: Load and Prepare Data\n","\n","**Purpose**: Download and prepare the FashionMNIST dataset for training and testing.\n","\n","**What is FashionMNIST?**\n","- 70,000 grayscale images of clothing items (28x28 pixels)\n","- 10 classes: T-shirt, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot\n","- Split into 60,000 training and 10,000 test images\n","\n","**Key Parameters**:\n","- `root=\"data\"` - Directory to store the dataset\n","- `train=True/False` - Specify training or test set\n","- `download=True` - Automatically download if not present\n","- `transform=ToTensor()` - Convert PIL images to PyTorch tensors (0-1 range)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Kk54wRO_hK1"},"outputs":[],"source":["\n","train_dataloader = DataLoader(training_data, batch_size=64)\n","test_dataloader = DataLoader(test_data, batch_size=64)\n"]},{"cell_type":"markdown","metadata":{"id":"1MbRM5UBaLtI"},"source":["## Step 3: Create DataLoaders\n","\n","**Purpose**: Wrap datasets in DataLoader for efficient batch processing during training.\n","\n","**Why DataLoader?**\n","- Automatically batches data into manageable chunks\n","- Shuffles data between epochs (for training)\n","- Enables parallel data loading with multiple workers\n","- Memory efficient - loads only the current batch\n","\n","**Batch Size = 64**: Each iteration processes 64 images at once, balancing memory usage and training speed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CVFtSCO_-8O6"},"outputs":[],"source":["\n","\n","class NeuralNetwork(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(28*28, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 10),\n","        )\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        logits = self.linear_relu_stack(x)\n","        return logits\n"]},{"cell_type":"markdown","metadata":{"id":"TT3y7e3WaLtJ"},"source":["## Step 4: Define the Neural Network\n","\n","**Purpose**: Create the model architecture that will learn to classify images.\n","\n","**Architecture Breakdown**:\n","1. **Input Layer**: 28Ã—28 = 784 pixels (flattened)\n","2. **Hidden Layer 1**: 784 â†’ 512 neurons with ReLU activation\n","3. **Hidden Layer 2**: 512 â†’ 512 neurons with ReLU activation\n","4. **Output Layer**: 512 â†’ 10 neurons (one per class)\n","\n","**Key Components**:\n","- `nn.Flatten()` - Converts 2D image (28Ã—28) to 1D vector (784)\n","- `nn.Linear()` - Fully connected layers (neurons)\n","- `nn.ReLU()` - Activation function (introduces non-linearity)\n","- `nn.Sequential()` - Chains layers together\n","\n","**Forward Pass**: Defines how data flows through the network from input to output."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":72,"status":"ok","timestamp":1768217998500,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"},"user_tz":-330},"id":"q-GWAPUm_jEG","outputId":"ea70d011-a1d3-402a-cc70-cc1e186b68af"},"outputs":[{"data":{"text/plain":["NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["\n","model = NeuralNetwork()\n","model"]},{"cell_type":"markdown","metadata":{"id":"34873-gTaLtK"},"source":["## Step 5: Instantiate the Model\n","\n","**Purpose**: Create an instance of the neural network.\n","\n","This creates an untrained model with randomly initialized weights. The model is now ready to be trained."]},{"cell_type":"markdown","metadata":{"id":"evsCpar3AoAQ"},"source":["## Hyperparameters"]},{"cell_type":"markdown","metadata":{"id":"NuQ6wKRnAtVd"},"source":["Hyperparameters are adjustable parameters that let you control the model optimization process. Different hyperparameter values can impact model training and convergence rates [read more](https://docs.pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html)\n","\n","We define the following hyperparameters for training:\n","* Number of Epochs - the number of times to iterate over the dataset\n","\n","* Batch Size - the number of data samples propagated through the network before the parameters are updated\n","\n","* Learning Rate - how much to update models parameters at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"czw_KuqKAhk2"},"outputs":[],"source":["learning_rate = 1e-3\n","batch_size = 64\n","epochs = 5"]},{"cell_type":"markdown","metadata":{"id":"VKNmOHq7aLtM"},"source":["## Step 6: Set Hyperparameters\n","\n","**Purpose**: Configure the training process parameters.\n","\n","**Hyperparameter Definitions**:\n","- **Learning Rate (1e-3 = 0.001)**: Controls how much to adjust weights after each batch\n","  - Too high â†’ unstable training\n","  - Too low â†’ slow learning\n","  \n","- **Batch Size (64)**: Number of samples processed before updating weights\n","  - Larger â†’ more stable gradients, more memory\n","  - Smaller â†’ faster updates, noisier gradients\n","  \n","- **Epochs (5)**: Number of times to iterate through the entire dataset\n","  - More epochs â†’ better learning (but risk overfitting)\n","\n","These are the \"knobs\" you tune to improve model performance!"]},{"cell_type":"markdown","metadata":{"id":"pKFoWCsXBFUj"},"source":["## Optimization Loop\n","Once we set our hyperparameters, we can then train and optimize our model with an optimization loop. Each iteration of the optimization loop is called an epoch.\n","\n","**Each epoch consists of two main parts:**\n","**The Train Loop** - iterate over the training dataset and try to converge to optimal parameters.\n","\n","**The Validation/Test Loop** - iterate over the test dataset to check if model performance is improving.\n","\n","Letâ€™s briefly familiarize ourselves with some of the concepts used in the training loop. Jump ahead to see the Full Implementation of the optimization loop."]},{"cell_type":"markdown","metadata":{"id":"O7FvMr4VBZUC"},"source":["## Loss Function\n","When presented with some training data, our untrained network is likely not to give the correct answer. Loss function measures the degree of dissimilarity of obtained result to the target value, and it is the loss function that we want to minimize during training. To calculate the loss we make a prediction using the inputs of our given data sample and compare it against the true data label value.\n","\n","Common loss functions include nn.MSELoss (Mean Square Error) for regression tasks, and nn.NLLLoss (Negative Log Likelihood) for classification. nn.CrossEntropyLoss combines `nn.LogSoftmax` and `nn.NLLLoss`.\n","\n","We pass our modelâ€™s output logits to `nn.CrossEntropyLoss`, which will normalize the logits and compute the prediction error."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IFr343xiBnip"},"outputs":[],"source":["# Initialize the loss function\n","loss_fn = nn.CrossEntropyLoss()"]},{"cell_type":"markdown","metadata":{"id":"Pfwo5VfHaLtO"},"source":["## Step 7: Initialize Loss Function\n","\n","**Purpose**: Measure how far the model's predictions are from actual labels.\n","\n","**CrossEntropyLoss**:\n","- Best for multi-class classification (choosing 1 class from many)\n","- Combines LogSoftmax + NLLLoss\n","- Lower loss = better predictions\n","\n","**How it works**:\n","- Compares predicted probabilities with true labels\n","- Penalizes confident wrong predictions more heavily\n","- Goal: Minimize this loss during training"]},{"cell_type":"markdown","metadata":{"id":"KygovxGyBq7w"},"source":["Optimizer\n","Optimization is the process of adjusting model parameters to reduce model error in each training step. Optimization algorithms define how this process is performed (in this example we use Stochastic Gradient Descent). All optimization logic is encapsulated in the optimizer object. Here, we use the SGD optimizer; additionally, there are many different optimizers available in PyTorch such as ADAM and RMSProp, that work better for different kinds of models and data.\n","\n","We initialize the optimizer by registering the modelâ€™s parameters that need to be trained, and passing in the learning rate hyperparameter."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hsnt4ylBBxIg"},"outputs":[],"source":["optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"]},{"cell_type":"markdown","metadata":{"id":"-0U_OkqtaLtQ"},"source":["## Step 8: Initialize Optimizer\n","\n","**Purpose**: Algorithm that updates model weights to minimize loss.\n","\n","**SGD (Stochastic Gradient Descent)**:\n","- Classic optimization algorithm\n","- Updates weights based on gradients (derivatives of loss)\n","- \"Descends\" toward minimum loss\n","\n","**Alternative Optimizers**:\n","- `Adam` - Adaptive learning rate, often faster convergence\n","- `RMSProp` - Good for recurrent networks\n","- `AdaGrad` - Adapts learning rate per parameter\n","\n","The optimizer needs:\n","1. Model parameters to update\n","2. Learning rate to control update size"]},{"cell_type":"markdown","metadata":{"id":"Xo1JnaBLBzjQ"},"source":["Inside the training loop, optimization happens in three steps:\n","Call optimizer.zero_grad() to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n","\n","Backpropagate the prediction loss with a call to loss.backward(). PyTorch deposits the gradients of the loss w.r.t. each parameter.\n","\n","Once we have our gradients, we call optimizer.step() to adjust the parameters by the gradients collected in the backward pass."]},{"cell_type":"markdown","metadata":{"id":"caenxh1zaLtR"},"source":["### The Three Steps of Optimization (per batch):\n","\n","**Visual Flow**:\n","```\n","Data Batch â†’ Forward Pass â†’ Calculate Loss â†’ Backward Pass â†’ Update Weights â†’ Repeat\n","```\n","\n","1. **`optimizer.zero_grad()`**\n","   - Clears old gradients from previous batch\n","   - Prevents gradient accumulation\n","\n","2. **`loss.backward()`**\n","   - Backpropagation: Calculates gradients for all parameters\n","   - Determines how to adjust each weight\n","\n","3. **`optimizer.step()`**\n","   - Updates weights using calculated gradients\n","   - Moves model toward better predictions\n","\n","**Remember**: Zero gradients â†’ Calculate gradients â†’ Apply gradients"]},{"cell_type":"markdown","metadata":{"id":"dAkjHsWeCLdh"},"source":["# Full Implementaion"]},{"cell_type":"markdown","metadata":{"id":"p_vJ64HvB7ju"},"source":["We define train_loop that loops over our optimization code, and test_loop that evaluates the modelâ€™s performance against our test data."]},{"cell_type":"markdown","metadata":{"id":"KIUAAg7IaLtT"},"source":["## Step 9: Define Training and Testing Functions\n","\n","**Purpose**: Encapsulate the training and evaluation logic.\n","\n","### `train_loop()` Function:\n","**What it does**: Trains the model for one epoch\n","- Loops through all training batches\n","- For each batch:\n","  1. Forward pass (get predictions)\n","  2. Calculate loss\n","  3. Backward pass (compute gradients)\n","  4. Update weights\n","  5. Zero gradients for next iteration\n","- Prints loss every 100 batches to monitor progress\n","\n","**Key setting**: `model.train()` - Enables training mode (activates dropout, batch norm)\n","\n","---\n","\n","### `test_loop()` Function:\n","**What it does**: Evaluates model performance on test data\n","- Loops through all test batches\n","- Calculates predictions without updating weights\n","- Computes accuracy and average loss\n","- Reports overall performance\n","\n","**Key settings**:\n","- `model.eval()` - Evaluation mode (disables dropout)\n","- `torch.no_grad()` - Disables gradient computation (saves memory/time)\n","\n","**Why separate test loop?**\n","- Prevents overfitting (model hasn't seen test data)\n","- Validates that model generalizes to new data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BUG1V08sB5_R"},"outputs":[],"source":["def train_loop(dataloader, model, loss_fn, optimizer):\n","    size = len(dataloader.dataset)\n","    # Set the model to training mode - important for batch normalization and dropout layers\n","    # Unnecessary in this situation but added for best practices\n","    model.train()\n","    for batch, (X, y) in enumerate(dataloader):\n","        # Compute prediction and loss\n","        pred = model(X)\n","        loss = loss_fn(pred, y)\n","\n","        # Backpropagation\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","        if batch % 100 == 0:\n","            loss, current = loss.item(), batch * batch_size + len(X)\n","            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n","\n","\n","def test_loop(dataloader, model, loss_fn):\n","    # Set the model to evaluation mode - important for batch normalization and dropout layers\n","    # Unnecessary in this situation but added for best practices\n","    model.eval()\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    test_loss, correct = 0, 0\n","\n","    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n","    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            pred = model(X)\n","            test_loss += loss_fn(pred, y).item()\n","            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","\n","    test_loss /= num_batches\n","    correct /= size\n","    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"]},{"cell_type":"markdown","metadata":{"id":"7cpssStVCTWh"},"source":["We initialize the loss function and optimizer, and pass it to train_loop and test_loop. Feel free to increase the number of epochs to track the modelâ€™s improving performance."]},{"cell_type":"markdown","metadata":{"id":"EC7V0dv4aLtV"},"source":["## Step 10: Train the Model\n","\n","**Purpose**: Execute the complete training process.\n","\n","**Training Process** (10 epochs):\n","- **Epoch** = One complete pass through training data\n","- For each epoch:\n","  1. Train on entire training set (`train_loop`)\n","  2. Evaluate on entire test set (`test_loop`)\n","  3. Print progress\n","\n","**What to Watch**:\n","- **Training Loss**: Should decrease over epochs\n","- **Test Accuracy**: Should increase over epochs\n","- **Gap between train and test**: If test accuracy plateaus while training improves, model may be overfitting\n","\n","**Expected Behavior**:\n","- Initial accuracy ~10% (random guessing among 10 classes)\n","- Final accuracy ~85-88% after 10 epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":136668,"status":"ok","timestamp":1768218705696,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"},"user_tz":-330},"id":"2PvjfTdzCRqH","outputId":"337e96b7-e992-4c67-c909-76f9f9691864"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1\n","-------------------------------\n","loss: 2.306856  [   64/60000]\n","loss: 2.298582  [ 6464/60000]\n","loss: 2.283126  [12864/60000]\n","loss: 2.264119  [19264/60000]\n","loss: 2.248101  [25664/60000]\n","loss: 2.226551  [32064/60000]\n","loss: 2.225410  [38464/60000]\n","loss: 2.193868  [44864/60000]\n","loss: 2.198018  [51264/60000]\n","loss: 2.159197  [57664/60000]\n","Test Error: \n"," Accuracy: 43.6%, Avg loss: 2.157352 \n","\n","Epoch 2\n","-------------------------------\n","loss: 2.168603  [   64/60000]\n","loss: 2.159842  [ 6464/60000]\n","loss: 2.107098  [12864/60000]\n","loss: 2.107127  [19264/60000]\n","loss: 2.065855  [25664/60000]\n","loss: 2.007953  [32064/60000]\n","loss: 2.022113  [38464/60000]\n","loss: 1.944845  [44864/60000]\n","loss: 1.959247  [51264/60000]\n","loss: 1.875185  [57664/60000]\n","Test Error: \n"," Accuracy: 58.2%, Avg loss: 1.881718 \n","\n","Epoch 3\n","-------------------------------\n","loss: 1.915567  [   64/60000]\n","loss: 1.884475  [ 6464/60000]\n","loss: 1.778140  [12864/60000]\n","loss: 1.798797  [19264/60000]\n","loss: 1.709102  [25664/60000]\n","loss: 1.660662  [32064/60000]\n","loss: 1.665707  [38464/60000]\n","loss: 1.577755  [44864/60000]\n","loss: 1.611143  [51264/60000]\n","loss: 1.493752  [57664/60000]\n","Test Error: \n"," Accuracy: 60.9%, Avg loss: 1.521906 \n","\n","Epoch 4\n","-------------------------------\n","loss: 1.588650  [   64/60000]\n","loss: 1.552963  [ 6464/60000]\n","loss: 1.417349  [12864/60000]\n","loss: 1.466972  [19264/60000]\n","loss: 1.369297  [25664/60000]\n","loss: 1.363701  [32064/60000]\n","loss: 1.360835  [38464/60000]\n","loss: 1.297623  [44864/60000]\n","loss: 1.338738  [51264/60000]\n","loss: 1.230714  [57664/60000]\n","Test Error: \n"," Accuracy: 63.0%, Avg loss: 1.261174 \n","\n","Epoch 5\n","-------------------------------\n","loss: 1.334185  [   64/60000]\n","loss: 1.316130  [ 6464/60000]\n","loss: 1.162555  [12864/60000]\n","loss: 1.248604  [19264/60000]\n","loss: 1.139457  [25664/60000]\n","loss: 1.163463  [32064/60000]\n","loss: 1.169330  [38464/60000]\n","loss: 1.118035  [44864/60000]\n","loss: 1.162610  [51264/60000]\n","loss: 1.072293  [57664/60000]\n","Test Error: \n"," Accuracy: 64.8%, Avg loss: 1.094573 \n","\n","Epoch 6\n","-------------------------------\n","loss: 1.159976  [   64/60000]\n","loss: 1.162498  [ 6464/60000]\n","loss: 0.990891  [12864/60000]\n","loss: 1.108602  [19264/60000]\n","loss: 0.995759  [25664/60000]\n","loss: 1.026743  [32064/60000]\n","loss: 1.049318  [38464/60000]\n","loss: 1.001662  [44864/60000]\n","loss: 1.045579  [51264/60000]\n","loss: 0.971820  [57664/60000]\n","Test Error: \n"," Accuracy: 65.7%, Avg loss: 0.985532 \n","\n","Epoch 7\n","-------------------------------\n","loss: 1.037144  [   64/60000]\n","loss: 1.061795  [ 6464/60000]\n","loss: 0.872420  [12864/60000]\n","loss: 1.013977  [19264/60000]\n","loss: 0.904201  [25664/60000]\n","loss: 0.929672  [32064/60000]\n","loss: 0.970969  [38464/60000]\n","loss: 0.925437  [44864/60000]\n","loss: 0.964134  [51264/60000]\n","loss: 0.904229  [57664/60000]\n","Test Error: \n"," Accuracy: 67.2%, Avg loss: 0.910909 \n","\n","Epoch 8\n","-------------------------------\n","loss: 0.946680  [   64/60000]\n","loss: 0.992131  [ 6464/60000]\n","loss: 0.787279  [12864/60000]\n","loss: 0.946767  [19264/60000]\n","loss: 0.843376  [25664/60000]\n","loss: 0.858648  [32064/60000]\n","loss: 0.916422  [38464/60000]\n","loss: 0.874524  [44864/60000]\n","loss: 0.905692  [51264/60000]\n","loss: 0.855784  [57664/60000]\n","Test Error: \n"," Accuracy: 68.4%, Avg loss: 0.857500 \n","\n","Epoch 9\n","-------------------------------\n","loss: 0.877950  [   64/60000]\n","loss: 0.940641  [ 6464/60000]\n","loss: 0.724049  [12864/60000]\n","loss: 0.896977  [19264/60000]\n","loss: 0.800490  [25664/60000]\n","loss: 0.805714  [32064/60000]\n","loss: 0.875859  [38464/60000]\n","loss: 0.839527  [44864/60000]\n","loss: 0.862727  [51264/60000]\n","loss: 0.819244  [57664/60000]\n","Test Error: \n"," Accuracy: 69.7%, Avg loss: 0.817508 \n","\n","Epoch 10\n","-------------------------------\n","loss: 0.823851  [   64/60000]\n","loss: 0.900259  [ 6464/60000]\n","loss: 0.675536  [12864/60000]\n","loss: 0.859106  [19264/60000]\n","loss: 0.768456  [25664/60000]\n","loss: 0.765385  [32064/60000]\n","loss: 0.843805  [38464/60000]\n","loss: 0.814111  [44864/60000]\n","loss: 0.830032  [51264/60000]\n","loss: 0.790279  [57664/60000]\n","Test Error: \n"," Accuracy: 70.9%, Avg loss: 0.786188 \n","\n","Done!\n"]}],"source":["loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","epochs = 10\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train_loop(train_dataloader, model, loss_fn, optimizer)\n","    test_loop(test_dataloader, model, loss_fn)\n","print(\"Done!\")"]},{"cell_type":"markdown","metadata":{"id":"qn__0QgYCxsl"},"source":["# Save and Load the Model"]},{"cell_type":"markdown","metadata":{"id":"OwBKw0PKaLtW"},"source":["## Step 11: Model Persistence\n","\n","**Purpose**: Save trained model for later use without retraining.\n","\n","### Saving the Model:\n","`torch.save(model, 'model.pth')`\n","- Saves entire model (architecture + weights)\n","- `.pth` or `.pt` are common PyTorch model file extensions\n","\n","**Alternative**: `torch.save(model.state_dict(), 'model.pth')`\n","- Saves only weights (smaller file, more flexible)\n","- Requires model architecture definition when loading\n","\n","---\n","\n","### Loading the Model:\n","`torch.load('model.pth', weights_only=False)`\n","- Loads entire saved model\n","- `weights_only=False` - Loads full model (not just weights)\n","- Model is ready to use for predictions immediately\n","\n","**Why Save Models?**\n","- Avoid retraining (saves time and compute)\n","- Deploy models to production\n","- Share models with others\n","- Resume training later"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Klxo5umEDs-6"},"outputs":[],"source":["torch.save(model, 'model.pth') # or model.state_dict()?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51,"status":"ok","timestamp":1768219093372,"user":{"displayName":"Chandan Kumar Barada","userId":"01468100662972031370"},"user_tz":-330},"id":"v0bN1IPVDyfh","outputId":"ddf5a6e1-f631-4fcf-e712-a78bfa08baa5"},"outputs":[{"data":{"text/plain":["NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["model = torch.load('model.pth', weights_only=False)\n"]},{"cell_type":"markdown","metadata":{"id":"MxaXZ2C-aLtf"},"source":["---\n","\n","## ğŸ¤” Which is Better: `torch.save(model)` vs `torch.save(model.state_dict())`?\n","\n","### âœ… **RECOMMENDED: `model.state_dict()`**\n","\n","**Advantages:**\n","- âœ… **More flexible** - Can load weights into different model versions\n","- âœ… **Smaller file size** - Only saves weights/parameters, not Python code\n","- âœ… **More portable** - Works across PyTorch versions better\n","- âœ… **Safer** - Avoids pickle security issues with untrusted files\n","- âœ… **Best practice** - Officially recommended by PyTorch documentation\n","- âœ… **Debugging friendly** - Can inspect weights separately\n","\n","**Usage Example:**\n","```python\n","# Saving\n","torch.save(model.state_dict(), 'model_weights.pth')\n","\n","# Loading (requires model architecture definition)\n","model = NeuralNetwork()  # Create model first\n","model.load_state_dict(torch.load('model_weights.pth', weights_only=True))\n","model.eval()  # Set to evaluation mode\n","```\n","\n","---\n","\n","### ğŸ”§ **Alternative: `torch.save(model)`**\n","\n","**When to use:**\n","- Quick prototyping and experimentation\n","- You need to save the entire architecture with weights\n","- You're sure the model class won't change\n","\n","**Advantages:**\n","- âœ… **Simpler loading** - No need to recreate architecture\n","- âœ… **One-liner** - Quick for testing\n","\n","**Disadvantages:**\n","- âŒ **Larger files** - Saves entire model structure\n","- âŒ **Less portable** - Can break if model class definition changes\n","- âŒ **Pickle dependency** - Security concerns with untrusted files\n","- âŒ **Version compatibility issues** - May not work across PyTorch versions\n","\n","**Usage Example:**\n","```python\n","# Saving\n","torch.save(model, 'model_full.pth')\n","\n","# Loading (simpler, but less flexible)\n","model = torch.load('model_full.pth', weights_only=False)\n","model.eval()\n","```\n","\n","---\n","\n","### ğŸ“Š Quick Comparison Table:\n","\n","| Feature | `state_dict()` | Full Model |\n","|---------|---------------|------------|\n","| **File Size** | Smaller âœ… | Larger âŒ |\n","| **Flexibility** | High âœ… | Low âŒ |\n","| **Portability** | Better âœ… | Worse âŒ |\n","| **Loading** | Requires architecture | Direct load âœ… |\n","| **Best Practice** | Yes âœ… | No âŒ |\n","| **Security** | Safer âœ… | Pickle risks âŒ |\n","\n","---\n","\n","### ğŸ’¡ **Recommendation for Production:**\n","\n","Always use `model.state_dict()` for:\n","- Production deployments\n","- Sharing models with others\n","- Long-term storage\n","- Model versioning\n","\n","Use full model save only for quick local experiments where convenience matters more than best practices."]},{"cell_type":"markdown","metadata":{"id":"-FdT76JYaLtg"},"source":["---\n","\n","## ğŸ“ Summary: Complete PyTorch Workflow\n","\n","### The 11-Step Process:\n","\n","1. âœ… **Import** libraries\n","2. âœ… **Load** datasets\n","3. âœ… **Create** DataLoaders\n","4. âœ… **Define** model architecture\n","5. âœ… **Instantiate** model\n","6. âœ… **Set** hyperparameters\n","7. âœ… **Initialize** loss function\n","8. âœ… **Initialize** optimizer\n","9. âœ… **Define** train/test loops\n","10. âœ… **Train** the model\n","11. âœ… **Save/Load** model\n","\n","### Key Concepts to Remember:\n","\n","- **Epochs**: Full passes through the dataset\n","- **Batches**: Subsets of data processed together\n","- **Loss**: Measure of prediction error (lower is better)\n","- **Optimizer**: Updates weights to minimize loss\n","- **Forward Pass**: Data â†’ Model â†’ Predictions\n","- **Backward Pass**: Loss â†’ Gradients â†’ Weight Updates\n","- **Train/Eval Modes**: Different behaviors for training vs testing\n","\n","### This workflow applies to most deep learning problems!\n","Just swap out the dataset, model architecture, and hyperparameters for your specific task.\n","\n","---\n","**Next Steps**: Experiment with different architectures, optimizers, or datasets to deepen understanding!"]},{"cell_type":"markdown","metadata":{"id":"P930l8-GaLth"},"source":["---\n","\n","# ğŸ” Making Predictions with the Trained Model\n","\n","Now that we have a trained model, let's see how to actually use it for making predictions on individual samples. This section demonstrates the complete inference pipeline.\n","\n","## What We'll Do:\n","1. Take sample images from the test dataset\n","2. Feed them to the trained model\n","3. Examine raw model outputs (logits)\n","4. Convert logits to predicted class labels\n","5. Compare predictions with actual labels\n","\n","This is essential for understanding how your model performs on real data!"]},{"cell_type":"markdown","metadata":{"id":"mJoID9MlaLti"},"source":["## Step 12: Define Class Labels\n","\n","**Purpose**: Map numeric class indices to human-readable labels.\n","\n","FashionMNIST has 10 classes (0-9). We need a mapping to understand what the model is predicting."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OHMaALxdaLti"},"outputs":[],"source":["# FashionMNIST class labels\n","class_names = [\n","    \"T-shirt/top\",\n","    \"Trouser\",\n","    \"Pullover\",\n","    \"Dress\",\n","    \"Coat\",\n","    \"Sandal\",\n","    \"Shirt\",\n","    \"Sneaker\",\n","    \"Bag\",\n","    \"Ankle boot\"\n","]\n","\n","print(\"FashionMNIST Classes:\")\n","for idx, name in enumerate(class_names):\n","    print(f\"  {idx}: {name}\")"]},{"cell_type":"markdown","metadata":{"id":"lyCKxXHTaLti"},"source":["## Step 13: Get Sample Data for Testing\n","\n","**Purpose**: Extract individual samples from the test dataset to make predictions.\n","\n","We'll get a few test images with their true labels to see how the model performs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wMQz_n4aaLtj"},"outputs":[],"source":["# Get 5 random samples from test dataset\n","import random\n","\n","# Set seed for reproducibility\n","random.seed(42)\n","sample_indices = random.sample(range(len(test_data)), 5)\n","\n","# Extract samples\n","sample_images = []\n","sample_labels = []\n","\n","for idx in sample_indices:\n","    image, label = test_data[idx]\n","    sample_images.append(image)\n","    sample_labels.append(label)\n","    print(f\"Sample {len(sample_images)}: Index {idx}, True Label: {label} ({class_names[label]})\")\n","\n","print(f\"\\nExtracted {len(sample_images)} samples for testing\")"]},{"cell_type":"markdown","metadata":{"id":"CJE7tuimaLtj"},"source":["## Step 14: Make Predictions (Single Sample)\n","\n","**Purpose**: Understand the prediction process step-by-step for ONE sample.\n","\n","### The Prediction Pipeline:\n","1. **Prepare Input**: Get image tensor\n","2. **Add Batch Dimension**: Model expects batches, even for single image\n","3. **Set Eval Mode**: Disable dropout/batch norm training behavior\n","4. **Forward Pass**: Feed image through model\n","5. **Get Logits**: Raw output scores (unnormalized)\n","6. **Get Probabilities**: Apply softmax to convert logits to probabilities\n","7. **Get Prediction**: Find class with highest probability"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GVQqCS4GaLtj"},"outputs":[],"source":["# Take the first sample\n","test_image = sample_images[0]\n","true_label = sample_labels[0]\n","\n","print(f\"True Label: {true_label} ({class_names[true_label]})\")\n","print(f\"Image Shape: {test_image.shape}\")  # Should be [1, 28, 28]\n","\n","# Step 1: Set model to evaluation mode\n","model.eval()\n","\n","# Step 2: Add batch dimension (model expects [batch_size, channels, height, width])\n","image_batch = test_image.unsqueeze(0)  # Shape: [1, 1, 28, 28]\n","print(f\"Batch Shape: {image_batch.shape}\")\n","\n","# Step 3: Make prediction (no gradient computation needed)\n","with torch.no_grad():\n","    # Forward pass - get raw outputs (logits)\n","    logits = model(image_batch)\n","\n","print(f\"\\n--- Raw Model Output (Logits) ---\")\n","print(f\"Logits Shape: {logits.shape}\")  # Should be [1, 10]\n","print(f\"Logits: {logits}\")\n","print(f\"\\nLogits are unnormalized scores for each class.\")\n","print(f\"Higher logit = model is more confident about that class.\")"]},{"cell_type":"markdown","metadata":{"id":"sjbjF-rwaLtj"},"source":["## Step 15: Convert Logits to Probabilities\n","\n","**Purpose**: Transform raw logits into interpretable probabilities using Softmax.\n","\n","**What is Softmax?**\n","- Converts logits into probabilities that sum to 1.0\n","- Higher logits â†’ higher probabilities\n","- Makes outputs easier to interpret (e.g., \"85% confident it's a T-shirt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PX5NLkzUaLtk"},"outputs":[],"source":["# Apply softmax to convert logits to probabilities\n","probabilities = torch.nn.functional.softmax(logits, dim=1)\n","\n","print(\"--- Probabilities (after Softmax) ---\")\n","print(f\"Probabilities Shape: {probabilities.shape}\")\n","print(f\"Probabilities: {probabilities}\")\n","print(f\"\\nSum of probabilities: {probabilities.sum().item():.4f} (should be 1.0)\")\n","\n","# Display probabilities for each class\n","print(\"\\n--- Probability for Each Class ---\")\n","for idx, prob in enumerate(probabilities[0]):\n","    print(f\"{idx}: {class_names[idx]:15} -> {prob.item()*100:6.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"E4Z0aJuVaLtk"},"source":["## Step 16: Get Final Prediction\n","\n","**Purpose**: Extract the predicted class (the one with highest probability).\n","\n","We use `argmax()` to find the index of the maximum value, which corresponds to the predicted class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EB0CpcMDaLtl"},"outputs":[],"source":["# Get the predicted class (index with highest probability)\n","predicted_class = logits.argmax(1).item()\n","confidence = probabilities[0][predicted_class].item()\n","\n","print(\"=\" * 50)\n","print(\"ğŸ¯ PREDICTION RESULT\")\n","print(\"=\" * 50)\n","print(f\"True Label:      {true_label} ({class_names[true_label]})\")\n","print(f\"Predicted Label: {predicted_class} ({class_names[predicted_class]})\")\n","print(f\"Confidence:      {confidence*100:.2f}%\")\n","print(f\"Correct:         {'âœ… YES' if predicted_class == true_label else 'âŒ NO'}\")\n","print(\"=\" * 50)"]},{"cell_type":"markdown","metadata":{"id":"ab_QSbq6aLtl"},"source":["## Step 17: Batch Predictions (Multiple Samples)\n","\n","**Purpose**: Make predictions for multiple samples at once (more efficient).\n","\n","Instead of processing images one by one, we can stack them into a batch for faster inference."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f-JgUxE4aLtm"},"outputs":[],"source":["# Stack all sample images into a batch\n","batch_images = torch.stack(sample_images)  # Shape: [5, 1, 28, 28]\n","print(f\"Batch Shape: {batch_images.shape}\")\n","\n","# Make batch predictions\n","model.eval()\n","with torch.no_grad():\n","    batch_logits = model(batch_images)\n","    batch_probabilities = torch.nn.functional.softmax(batch_logits, dim=1)\n","    batch_predictions = batch_logits.argmax(1)\n","\n","print(\"\\n\" + \"=\" * 70)\n","print(\"ğŸ“Š BATCH PREDICTION RESULTS\")\n","print(\"=\" * 70)\n","\n","# Display results for each sample\n","for i in range(len(sample_images)):\n","    true_label = sample_labels[i]\n","    pred_label = batch_predictions[i].item()\n","    confidence = batch_probabilities[i][pred_label].item()\n","\n","    is_correct = \"âœ…\" if pred_label == true_label else \"âŒ\"\n","\n","    print(f\"\\nSample {i+1}:\")\n","    print(f\"  True:       {true_label} ({class_names[true_label]})\")\n","    print(f\"  Predicted:  {pred_label} ({class_names[pred_label]})\")\n","    print(f\"  Confidence: {confidence*100:.2f}%\")\n","    print(f\"  Result:     {is_correct}\")\n","\n","# Calculate accuracy for this batch\n","correct = (batch_predictions == torch.tensor(sample_labels)).sum().item()\n","accuracy = correct / len(sample_labels) * 100\n","print(\"\\n\" + \"=\" * 70)\n","print(f\"Batch Accuracy: {correct}/{len(sample_labels)} = {accuracy:.1f}%\")\n","print(\"=\" * 70)"]},{"cell_type":"markdown","metadata":{"id":"KePGPatmaLtm"},"source":["## ğŸ“š Understanding the Output\n","\n","### Raw Logits vs Probabilities:\n","\n","**Logits** (Raw Output):\n","- Unnormalized scores from the final layer\n","- Can be any real number (negative or positive)\n","- Larger value = more confident\n","- Example: `[2.3, -1.5, 0.8, ...]`\n","\n","**Probabilities** (After Softmax):\n","- Normalized values between 0 and 1\n","- Sum to exactly 1.0\n","- Interpretable as confidence percentages\n","- Example: `[0.85, 0.02, 0.05, ...]` â†’ 85%, 2%, 5%, etc.\n","\n","### Why Use argmax()?\n","- `argmax()` finds the **index** of the maximum value\n","- For logits `[2.3, -1.5, 5.1, 0.8]`, `argmax()` returns `2` (the 3rd position)\n","- This index corresponds to the predicted class label\n","\n","### Key Takeaways:\n","1. **Always use `model.eval()`** before making predictions\n","2. **Use `torch.no_grad()`** to disable gradient computation (saves memory)\n","3. **Add batch dimension** even for single images: `image.unsqueeze(0)`\n","4. **Logits â†’ Softmax â†’ Probabilities â†’ argmax â†’ Predicted Class**\n","5. **Batch processing is faster** than processing one image at a time\n","\n","---\n","\n","### Quick Reference - Inference Pipeline:\n","```python\n","model.eval()\n","with torch.no_grad():\n","    logits = model(image_batch)                    # Raw scores\n","    probs = torch.nn.functional.softmax(logits, dim=1)  # Probabilities\n","    prediction = logits.argmax(1)                  # Predicted class\n","```"]},{"cell_type":"markdown","source":["```\n","â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n","â”‚         1. MODEL (Parameters)               â”‚\n","â”‚   Weights & Biases (~669K numbers)          â”‚\n","â”‚   Status: requires_grad=True                â”‚\n","â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","                    â†“\n","         Forward Pass (make predictions)\n","                    â†“\n","â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n","â”‚         2. AUTOGRAD (Gradients)             â”‚\n","â”‚   Tracks operations                         â”‚\n","â”‚   Computes: âˆ‚loss/âˆ‚parameter                â”‚\n","â”‚   Called via: loss.backward()               â”‚\n","â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","                    â†“\n","         Gradients stored in .grad\n","                    â†“\n","â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n","â”‚         3. OPTIMIZER (Updates)              â”‚\n","â”‚   Uses gradients to update parameters       â”‚\n","â”‚   New_weight = Old_weight - lr * gradient   â”‚\n","â”‚   Called via: optimizer.step()              â”‚\n","â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","```"],"metadata":{"id":"_9dW-3h8un2U"}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}